十三、Unsupervised Learning(无监督学习)
===
## Clustering(聚类)
---

## 13.1、 Unsupervised Learning:Introduction(无监督学习：介绍)

这是我们第一个 无监督学习算法。 我们要从未标记的数据中进行学习, 而不是从已标记的数据。 

那么, 何谓 无监督学习算法 ? 

将 无监督学习算法 与 监督学习算法 做个对照。 

首先，这是一个典型的监督学习例子。我们被给定一个带标签的训练集， 我们想要找出一条决策边界，藉此区分开 正(positive)或负(negative)标记数据。 

在这种 监督式学习 案例中。 我们针对一组附标记的训练数据 提出一个适当的假设并用假设函数去拟合它。 相比之下，在无监督学习案例中，我们得到的数据 看起来像这样：一个数据集， 一堆数据点，但数据之间没有任何标记以供参考。 

Training set:${x^{(1)},x^{(2)},x^{(3)},...,x^{(n)}}$

所以从训练数据中， 我们只能看到 x 1、x 2...到x(m),没有任何标记 y 供参考

所以在无监督学习中， 我们将这种无标签的训练数据送入 特定的算法， 然后我们要求算法 替我们分析出数据的结构。 

![13.1.1]()

就上图数据而言， 其中一种可能的结构 是 所有的数据 可以大致地划分成两组分开的点集(簇)。

这种能够找出我所圈出的这些簇的算法被称为聚类算法

这是我们第一种 无监督学习算法。 除此之外，无监督学习还包含很多其他类型的算法，他们能找出其他类型的结构或者数据的其他类型的模式，而不单单是簇，目前，我只讨论 聚类 。 

那么，聚类功用为何？ 

![13.1.2]()

稍早前，我已经提到几个应用实例： 

一、细分市场(Market segmentation)。 假设有一个客户数据库， 想要将所有客户划分 至不同的细分市场组， 以便于营销 或服务。 

二、社交网络分析(Social network analysis) 比如， 观察一群人，社交网络 像 Facebook、 Google+ 或者有关 个人的信息... 你通常和谁有电子邮件来往，. 他们又和谁有电子邮件来往。. 或者 查找一群相互有联系的人。 

三、利用聚类算法来组织计算机集群以及更好地组织数据集(Organize computing clusters) 。 因为，如果你知道 在集群中，哪些计算机 的数据中心倾向于一起工作。 你可以用它重新组织 您的资源， 网络的布局， 优化数据中心和通信数据。 

四、使用聚类算法，试图理解星系的形成和其中的天文的细节(Astronomical data analysis)。 

## 13.2、 K-Means Algorithm(K-Means 算法)

在聚类问题中 我们有未加标签的数据 我们希望有一个算法 能够自动的 把这些数据分成 有紧密关系的子集或是簇 

K均值 (K-means) 算法是现在最为广泛使用的聚类方法，那么在这个视频中我将会告诉你：什么是K均值算法以及它是怎么运作的 

K均值算法最好用图来表达 如图所示

![13.2.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/wUnSMmqOvIdRe8aso1tSic2iHdA230WBpl7H1Cj*yBg!/b/dN4AAAAAAAAA&bo=tAK3AQAAAAARFyA!&rf=viewer_4)

现在我有一些 没加标签的数据 而我想将这些数据分成两个簇 

现在我执行K均值算法 方法是这样的 

首先我随机选择两个点(我想要聚出2个类(簇)))，这两个点叫做聚类中心 (cluster centroids) 就是图上边的两个叉

K均值是一个迭代方法 它要做两件事情 

第一个是簇分配 第二个是移动聚类中心

在K均值算法的每次循环中 第一步是要进行簇分配： 我要遍历所有的样本 就是图上所有的绿色的点 然后依据 每一个点 是更接近红色的这个中心 还是蓝色的这个中心 来将每个数据点分配、归属到两个不同的聚类中心中 

具体来讲是对数据集中的所有点，依据他们更接近红色这个中心还是蓝色这个中心，进行染色，染色之后的结果如图所示

![13.2.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/ZoNXfjmoAI2BRsu81K8p8N*YolhCq2Axog0U1CxW7mo!/b/dAsBAAAAAAAA&bo=uQK3AQAAAAARFy0!&rf=viewer_4)

以上就是簇分配的步骤 

K均值的第二步 是要移动聚类中心: 我们将两个聚类中心，也就是说红色的叉和蓝色的叉移动到和它一样颜色的那堆点的均值处，也就是找出所有红色的点 计算出它们的均值(平均下来的位置) 然后我们就把红色点的聚类中心移动到这里 蓝色的点也是这样

现在两个中心都已经移动到新的均值那里了 你看 蓝色的这么移动 红色的这么移动 然后我们就会进入下一个 簇分配 我们重新检查所有没有标签的样本，依据它离红色中心还是蓝色中心更近一些 将它染成红色或是蓝色 我要将每个点 分配给两个中心的某一个，一步步地循环，直到最后如果你从这一步开始继续迭代下去 聚类中心不再变 并且 那些点的颜色也不会变 在这时 我们就能说 K均值方法已经收敛了 在这些数据中找到两个簇 

我们用更加规范的格式描述K均值算法 

![13.2.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/6Z7gBFzF3zaVWMOr6J5O2SHdZY43.SR095VLbT6wJm4!/b/dPQAAAAAAAAA&bo=MgPDAQAAAAARF9M!&rf=viewer_4)

K均值算法接受两个输入 第一个是参数K 表示你想从数据中 聚类出的簇的个数 

第二个是只有x没有标签y的训练集，因为这是非监督学习

在非监督学习的 K均值算法里 我们约定 x(i) 是一个n维向量

这就是K均值算法 

第一步是 随机初始化 K 个聚类中心 记作 $μ1\ μ_2\ ...\ μ_k$,就像之前图中所示聚类中心对应于红色叉和蓝色叉所在的位置,于是我们有两个聚类中心,按照这样的记法 红叉是 $μ_1$ 蓝叉是 $μ_2$ 通常情况下 我们可能会有更多的聚类中心 

K均值的内部循环 是这样的 我们会重复做下面的事情 

首先 对于每个训练样本 我们用变量 $c^{(i)}$ 表示(第一个到第k个最接近x^{(i)}的聚类中心)K个聚类中心中最接近 $x^{(i)}$ 的 那个中心的下标。

这就是簇分配步骤这个步骤 我将每个样本 依据它离哪个聚类中心更近一些 将其染成对应的颜色 所以 c(i) 是一个 在1到 K 之间的数 而且它表明 这个点到底是 更接近红色叉 还是蓝色叉 

另一种表达方式是 $c^{(i)} = \begin{Vmatrix}
    x^{(i)} - μ_k
\end{Vmatrix}$,
我想要计算 c(i) 那么 我要用第i个样本x(i) 然后 计算出这个样本 距离所有K个聚类中心的距离 这是 μ 以及小写的k,大写的 K 表示 所有聚类中心的个数 小写的 k 则是 不同的中心的下标

我希望的是 在所有K个中心中 找到一个k 使得$x^{(i)}$到$μ_{(k)}$的距离 是$x^{(i)}$到所有的聚类中心的距离中 最小的那个 也就是说 k的值使这个最小,然后将这个最小的k值赋值给$c^{(i)}$

这里还有另外的表示$c^{(i)}$的方法

我用xi减μk的范数 来表示 

$\begin{Vmatrix}
    x^{(i)}-μ_k
\end{Vmatrix}$

这是第i个训练样本 到聚类中心μk 的距离 注意 我这里用的是小写的k 大写的K 大写的k表示 聚类中心的总数 这个小写的k 是第一个到第K个中心 中的一个 我用小写的k 表示不同聚类中心的下标索引 

这就是某个样本到聚类中心的距离 接下来 我要做的是 找出一个k值 使得这个式子最小，这个k值就是我要赋值给$c^{(i)}$的值

按照惯例我这里写的这项 x(i) 和聚类中心的距离 我们惯例 用距离的平方来表示 所以我们可以认为 c(i)是距样本 x(i) 的距离的平方最小的那个聚类中心。

当然 使距离的平方最小或是距离最小 都能让我们得到相同的 c(i) 但是我们通常还是 写成距离的平方 因为这是约定俗成的 这就是簇分配 

K均值循环中的另一部分是 移动聚类中心 

它所做的是，对于每个聚类中心 也就是对于k从1到K 将 μk 赋值为这个簇所有点的均值 

举个栗子 某一个聚类中心 比如说是 μ2 被分配了一些训练样本 像是1,5,6,10 这表明 c(1)等于2 
c(5) 等于2 
c(6) 等于2 同样的 c(10) 也是等于2  

如果我们从上一步 也就是簇分配那一步得到了这些 这个表明 样本1 5 6 10被分配给了聚类中心2 

然后在移动聚类中心这一步中 我们要做的是 计算出这四个的平均值 

即 计算 x(1)+x(5)+x(6)+x(10) 然后计算 它们的平均值 这里聚类中心有 4个点 那么我们要计算和的四分之一 这时μ2就是一个 n维的向量 因为 x(1) x(5) x(6) x(10) 都是 

n维的向量 然后 把这些相加 再除以4 因为 有4个点分配到了这个聚类中心，这样就将μ2移动到 这四个点的均值处 

我要问的问题是 既然我们要让μk移动到分配给它的那些点的均值处 那么如果 存在一个 没有点分配给它的聚类中心 那怎么办? 通常在这种情况下 我们就直接移除 那个聚类中心 如果这么做了 最终将会得到K-1个簇 

而不是K个簇 如果就是要K个簇 不多不少 但是有个 没有点分配给它的聚类中心 你所要做的是 重新随机找一个聚类中心 但是直接移除那个中心 是更为常见的方法

K均值的 另外一个常见应用 应对没有很好分开的簇 

![13.2.4](http://a1.qpic.cn/psb?/V12umJF70r2BEK/TCAsoRtEn74Tkm2.IfOHqeaPdzBXaZSSeYhDHxVLwyQ!/b/dPQAAAAAAAAA&ek=1&kp=1&pt=0&bo=HAO5AQAAAAARF4c!&tl=3&vuin=904260897&tm=1536134400&sce=60-2-2&rf=viewer_4)

比如说 到目前为止 我们的K均值算法 都是基于一些像图中所示的数据 有很好的隔离开来的 三个簇 然后我们就用这个算法找出三个簇 但是事实是 K均值经常会用于 一些这样的数据 看起来并没有 很好的分来的 几个簇 

这是一个应用的例子 关于T恤的大小 

假设你是T恤制造商 你找到了一些人 想把T恤卖给他们 然后 你搜集了一些 这些人的 身高和体重的数据,就像这个图所表示的 然后你想确定一下T恤的大小 假设我们要设计 三种不同大小的t恤 小号 中号 和大号 那么小号应该是多大的? 中号呢? 大号呢? 

有一种方法就是在这样的数据上 使用K均值算法进行聚类，K均值可能将这些 聚成三个簇 所以说 尽管这些数据 原本看起来并没有 三个分开的簇 但是从某种程度上讲 K均值仍然能将数据分成几个类 

然后你能做的就是 看这第一群人 然后 查看他们的 身高和体重 试着去设计 对这群人来说 比较合身的小号衣服 以及设计一个中号的衣服 设计一个大号的衣服 这就是一种 市场细分的例子 

当你用K均值方法 将你的市场分为三个不同的部分 你就能够区别对待 你三类不同的顾客群体 更好的适应 他们不同的需求 就像大中小三种不同大小的衣服那样 这就是K均值算法 而且你现在应该 已经知道如果去实现 K均值算法并且利用它解决一些问题

## 13.3、 Optimization Objective(优化目标)

在大多数我们已经学到的 监督学习算法中 类似于线性回归 逻辑回归 以及更多的算法 所有的这些 算法都有一个优化目标函数 或者某个代价函数需要通过算法进行最小化 

事实上K均值也有一个优化目标函数或者需要最小化的代价函数，这节我会 告诉大家这个优化目标函数是什么 我这么做有两方面的目的 具体来说 

首先 了解什么是 K均值的优化目标函数 这将能帮助我们 调试学习算法 确保K均值算法 是在正确运行中 

第二个也是最重要的一个目的是我们该怎样运用这个来 帮助K均值找到更好的簇 并且避免局部最优解

另外顺便提一下 当K均值正在运行时 

我们将对两组变量进行跟踪 首先是 c(i) 它表示的是 当前的样本 x(i) 所归为 的那个簇的索引或者序号 另外一组变量 我们用 μk 来表示 第 k 个簇的  聚类中心 (cluster centroid) 顺便再提一句 K均值中我们用大写 K 来表示簇的总数 用小写 k 来表示 聚类中心的序号 因此 小写 k 的范围 就应该是1到大写K之间 

除此以外  还有另一个符号 我们用 $μ_{c^{(i)}}$ 来表示 x(i) 所属的那个簇 的聚类中心 

即，如果 x(i) 被划为了 第5个簇，x(i) 的序号 也就是 $c^{(i)} = 5$,也就表示x(i) 这个样本 被分到了第五个簇 因此 $μ_{c^{(5)}} = μ_5$ 因为 $c^{(i)} = 5$ 

所以 这里的 μc(i) 就是第5个簇的聚类中心 而也正是我的样本 x(i) 所属的第5个簇 

![13.3.1](http://a1.qpic.cn/psb?/V12umJF70r2BEK/eqMTQgQXi6Rw7f41GawMgEr8LkGhhBbdDmJaYd1VplQ!/b/dPQAAAAAAAAA&ek=1&kp=1&pt=0&bo=MQQ*AgAAAAARFyg!&tl=3&vuin=904260897&tm=1536138000&sce=60-2-2&rf=viewer_4)

有了这样的符号表示 现在我们就能写出 K均值聚类算法的 优化目标了 以下便是 K均值算法需要 最小化的代价函数 

$$J(c^{(1)},...,c^{(m)},μ_1,...,μ_K) = \frac{1}{m} \sum^m_{i=1} \begin{Vmatrix}
    x^{(i)}-μ_{c^{(i)}}
\end{Vmatrix}^2 $$

$J$ 参数是 $c^{(1)}$ 到 $c^{(m)}$ 以及 $μ_1$ 到 $μ_k$随着算法的执行过程 这些参数将不断变化 右边给出了优化目标 也就是所有的 1/m 乘以 i = 1 到 m 个项的求和 

这里我用红色框出了这部分 也即每个样本 x(i) 到 x(i) 所属的 聚类簇的中心 距离的平方值 

//下面 我来解释一下 这是训练样本 x(i) 的位置 这是 x(i) 这个样本被划分到的 聚类簇的中心的位置 我们在图上解释一下 如果这是 x1 x2 并且如果这个点 是我的第 i 个样本  x(i) 那么 也就是说这个值等于 x(i) 并且 x(i) 被分到了 某一个聚类中心 我用一个叉来表示这个聚类中心 所以 如果我们假设 这个聚类中心是 μ5  也就是说 假如 x(i) 被分到第五个聚类簇 那么这个距离平方值 也就是点 x(i) 和 x(i) 被分配到的聚类中心的 距离的平方值 //


那么 K均值算法 要做的事情就是 它将找到能够最小化 代价函数 J 的 c(i) 和 μi

这个代价函数 在K均值算法中 有时候也叫做 失真代价函数(distortion cost function) 或者K均值算法的失真



再说些细节

![13.3.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/pXjtP2D*qbbPAQzN*qmSZxF18eNwUnIiOFWXQNr8HwI!/b/dOAAAAAAAAAA&bo=GAQ6AgAAAAARFwQ!&rf=viewer_4)

这是K均值算法 这个算法的第一步 就是聚类中心的分配(簇分配) 在这一步中 我们要把每一个点 划分给各自所属的聚类中心 这个聚类簇的划分步骤 实际上就是在 对代价函数 J 进行最小化 它的参数是$c^{(1)},c^{(2)},...,c^{(m)} $,而我们要保持最近的聚类中心 μ1 到 μk 的位置固定不变 

因此 第一步要做的 其实不是改变 聚类中心的位置 而是选择 c(1) c(2) 一直到 c(m) 来最小化这个代价函数 或者说失真函数 J 

这个过程就是把这些点 划分到离它们最近的 那个聚类中心 因为这样才会使得 点到对应聚类中心的距离的平方最短 

然后是K-均值算法的第二步 

这一部分的任务是 聚类中心的的移动 也就是说这一步 是选择了能够 最小化 J 的 μ 的值 

也就是说最小化 代价函数J关于所有聚类中心的位置μ1到μK因此 K均值算法 实际上是把这两组变量 在这两部分中 分割开来考虑 分别最小化 J 首先是 c 作为变量 然后是μ作为变量 那么 K均值的工作就是 首先关于 c 求 J 的最小值 然后关于μ求J的最小值 然后反复循环 


这就是 K均值算法 

现在 我们已经理解了 K均值算法的原理 就是最小化代价函数 J 的过程 我们也可以用这个原理 来试着调试我们的学习算法 保证我们对 K均值算法的实现过程 是正确的 

好的 这节课我们介绍了 K均值算法 其核心就是 对代价函数 J 的优化过程 代价函数 J 也被称为失真函数 

我们可以用这个知识 来调试K均值算法 证明算法是否正在收敛 是否正在正常工作 

在下一节 我们将一起看看如何帮助K均值找到更好的簇，同时避免局部最优解 


## 13.4、 Random Initialization(随机初始化)

在本节课的视频中 讨论一下如何初始化K均值聚类方法,更重要的是 这将 引导我们讨论 如何避开局部最优来构建K均值聚类方法 

这是一个我们之前讨论过的 K均值聚类算法 

其中我们之前没有 讨论得太多的是这一步:如何初始化聚类中心,有几种不同的方法可以用来随机初始化聚类中心,但是事实证明有一种方法比其他 大多数可能考虑到的方法 更加被推荐 接下来就告诉你这个方法,因为它可能是效果最好的一种方法 

这里展示了我通常是如何初始化我的聚类中心的 

![13.4.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/3VRW8eYzTYT8igbe32z7*zFuF0uv9.JgS*f8xDiyvgA!/b/dAoBAAAAAAAA&bo=*QM0AgAAAAARF.g!&rf=viewer_4)

当运行K均值方法时 你需要有一个聚类中心数值K K值要比训练样本的数量m小 如果运行一个 K均值聚类中心数值等于或者大于样本数的K均值聚类方法会很奇怪 

我通常用来初始化K均值聚类的方法是：随机挑选K个训练样本,然后我要做的是设定μ1 到μk让它们等于这个K个样本 

我们假设 K 等于2 我们想找到两个聚类，那么为了初始化聚类中心，我要做的是：随机挑选几个样本。 我要初始化聚类中心的方法就只需要将这两个样本作为聚类中心,这就是一个随机初始化K均值聚类的方法

有的时候初始化的案例可能没有第一图那样完美，也许我最后 会挑选到两个样本的距离非常近，我们随机挑选了两个训练样本，并将这两个样本点作为初始化聚类中心.

因此在初始化时 你的第一个 聚类中心$μ_1 = x^{(i)}$ 对于某一个随机的$i$值,$μ_2 = x^{(j)}$ 对应另一个随机选择的不同的$j$的值等等 如果你有更多的聚类和更多的聚类中心的话就以此类推。

另外，K均值方法最终可能会得到的不同的结果 取决于 聚类簇的随机初始化方法，尤其是如果K均值方法落在局部最优的时候 

---

如果给你一些数据如图，这看起来好像有 3个聚类 那么 如果你运行K均值方法 如果 它最后得到一个 真正的全局最优 你可能会得到这样的聚类结果 

![13.4.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/BTyOu12tnpdq*5yfSR2.cR3E3lvHwCBkBhF4DGIMc.Y!/b/dPQAAAAAAAAA&bo=EwRfAgAAAAARF2o!&rf=viewer_4)

但是如果你运气特别不好，随机初始化K均值方法也可能会卡在不同的局部最优上面，如图中下面的两个例子。

这个局部最优项代表这个失真函数J的局部最优,这些在右下方的解 这些局部最优所对应的是真正的K均值方法所遇到的局部最优,且通过最小化这个失真函数J并不能得到很好的结果 

因此 如果你担心K均值方法会遇到 局部最优的问题 如果 你想提高 K均值方法找到最有可能的聚类的几率的话 我们能做的是尝试多次随机的初始化 而不是仅仅初始化一次K均值方法 就希望它会得到 很好的结果 我们能做的是 初始化K均值很多次 并运行K均值方法很多次 通过多次尝试 来保证我们最终能得到一个足够好的结果,一个尽可能局部或全局最优的结果 

具体地 这就是你能够做的 假如我决定运行 K均值方法一百次 那么我就需要执行这个循环 100次 

假设说有决定运行K均值方法100次 

那么这就意味这 我们要随机初始化K均值方法 对于这些100次随机初始化的每一次,我们需要运行K均值方法 我们会得到一系列聚类结果和一系列聚类中心 之后 我们可以计算失真函数J用我们得到的这些聚类结果和聚类中心来计算这样一个结果函数 

最后 完成整个过程100次之后 你会得到这个100种聚类数据的这些方法  最后你要做的是在所有这100种用于聚类的方法中 选取能够给我们代价最小的一个.

![13.4.3](http://a3.qpic.cn/psb?/V12umJF70r2BEK/YGWAKEfP74ce10ZM6f4EvwjLj6O5VZQz4ktkgMCf1lI!/b/dNoAAAAAAAAA&ek=1&kp=1&pt=0&bo=*wMwAgAAAAARF.4!&tl=3&vuin=904260897&tm=1536148800&sce=60-2-2&rf=viewer_4)

事实证明 如果你运行K均值方法时 所用的聚类数相当小 那么如果聚类 数是从 2到10之间的任何数的话 做多次的随机初始化 通常能够保证 你能有一个较好的局部最优解 保证你能找到更好的聚类数据 

但是如果K非常大的话 如果K比10大很多 当然如果K是成百上千个 如果你像找到成千上百个聚类,那么有多个随机初始化就不太可能会有太大的影响 更有可能你的第一次随机初始化就会给你相当好的结果 

多次随机 初始化可能会给 你稍微好一点的结果 但是不会好太多 但是在这样一个 聚类数相对较小的体系里 特别是如果你 有2个或者3个 或者4个聚类的话 随机 初始化会有较大的影响 可以保证你很好地最小化畸变函数并且能得到一个很好的聚类结果。

这就是随机初始化 的K均值初始化方法 

如果你尝试学习一种 聚类数目相对较小 的聚类方法 如2,3 4,5,6,7 用多次随机初始化 有时能够帮助你找到更好的数据聚类结果 但是 尽管你有很多聚类数目, 我在这里介绍的随机初始化方法也能给K均值方法一个合理的起始点来开始 并找到一个好的聚类结果 

## 13.5、 Choosing the Number of Clusters(选取聚类数量)

这一节我想讨论一下 K-均值聚类的最后一个细节 选择聚类数目的方法

这个问题其实没有一个非常标准的解答或者能自动解决它的方法，目前用来决定聚类数目的最常用的方法仍然是通过看可视化的图，或者看聚类算法的输出结果，或者其他一些东西来手动地决定聚类的数目。

但是 我确实经常被别人问到 这样的问题 你是如何来选择聚类的数目的 我只能告诉你一些 人们现在对这个问题的思考 尽管最为常见的方法 实际上仍是手动选择 聚类的数目。

选择聚类的数目可能不总是那么容易，大部分原因是：数据集中有多少个聚类通常是模棱两可的 

![13.5.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/5gn4pQJj8aCPRJCn2oTqYMFYQbqxw1UwAI*thhT0Re4!/b/dGwBAAAAAAAA&bo=cQX*AgAAAAARF6k!&rf=viewer_4)

看到这样一个数据集 有些人可能会看到 四个聚类 那么这就意味着需要使用 K=4 或者有些人可能 会看到两个聚类 这就意味着 K=2 可能其他人会看到3个聚类 

那么看到这样一个数据集,在我看来它的真实的类别数实际上确实是模棱两可的，所以我并不认为这里有一个正确答案 这就是无监督学习的一部分 没有给我们标签，所以不会总有一个清晰的答案 这是为什么做一个能够自动选择聚类数目的算法是非常困难的原因之一 

当人们讨论选择聚类数目的方法时可能会提及一个叫做肘部法则 (Elbow Method) 的方法，现在我来介绍一下它 之后会提及到它的一些优点和缺点 那么对于肘部法则 我们所需要做的是改变K的值 也就是聚类类别的总数 

我们用K值为1来运行K-均值聚类算法 这就意味着 所有的数据都会分到一个类里 然后计算代价函数 或者说计算畸变J 并将其画在这儿 然后我们选用两个聚类 来运行K-均值聚类算法 可能用了多个随机的初始中心 也可能没用 那么有两个聚类的话 我们很可能得到一个较小的畸变值，把它画在这儿。再使用三个、四个、五个聚类数目。

最后我们就能 得到一条展示了随着聚类数量的增多 畸变值是如何下降的曲线。 

![13.5.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/DodJ4irVUyKO9BUbhrQf06htVck1CdNdVT*HAxVBzew!/b/dAsBAAAAAAAA&bo=egUPAwAAAAARF1M!&rf=viewer_4)

看到这条曲线 肘部法则会说 “我们来看这个图 这里看起来是一个很清楚的肘点” 这就类比于人的手臂 这就类比于人的手臂 如果你想象一下 伸出你的手臂 那么这就是你的肩关节 这就是你的肘关节 我想你的手就在末端这里 这就是肘部法则 

你会发现这样一种模式 K从1变化到2 再从2到3时 畸变值迅速下降 然后在3的时候 到达一个肘点 此后畸变值就下降得非常慢 这样看起来 也许使用3个类 是聚类数目的正确选择 这是因为那个点 是曲线的肘点 就是说畸变值快速地下降 直到K等于3这个点 在这之后就下降得非常慢 那么我们就选K等于3 

当你应用肘部法则的时候 如果你得到了一个 像左边这样的图 那么这非常好 这是一种用来 选择聚类个数的合理方法 

而事实证明肘部法则 并不那么常用 其中一个原因是 如果你把这种方法 用到一个聚类问题上 事实上你最后得到的曲线 通常看起来 是更加模棱两可的 也许就像右图这样 

如果你看这条曲线 我不知道 也许没有一个清晰的肘点 而畸变值像是连续下降的 也许3是一个好选择 也许4是一个好选择 也许5也不差 那么如果 你在实际应用中使用了这个 如果你的图像左边这个的话 那么就太好了 它给你一个清晰的答案 但是通常来说 你最后得到的图 是像右边的这样的 肘点的位置 并不明确 这使得用这个方法 来选择聚类数目变得较为困难 

简单小结一下肘部法则 它是一个值得尝试的方法 但是我不会期待它在任何问题上都有很高的表现

最后 有另外一种方法 来考虑如何选择K的值 通常人们使用 K-均值聚类算法 是为了某些后面的用途 或者说某种下游的目的 而要求得一些聚类 

也许你会用K-均值聚类算法 来做市场分割 如我们之前谈论的T恤尺寸的例子 也许你会用K-均值聚类来让 电脑的聚类变得更好 或者可能为了某些别的目的学习聚类 等等 或者可能为了某些别的目的学习聚类 等等 如果那个后下游的目的 比如市场分割 那能给你一个评估标准 

那么通常来说 决定聚类数量的 更好的办法是 看不同的聚类数量能为 后续下游的目的提供多好的结果 

![13.5.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/JEbTGlGiXaPaWNxCSktjNK0okxpkOrH7adsMAdeoymI!/b/dA0BAAAAAAAA&bo=bgX7AgAAAAARF7I!&rf=viewer_4)

我们来看一个具体的例子 

我们再看一下 T恤尺寸这个例子 我想要决定 我是需要3种T恤尺寸么吗？ 所以我选择 K=3 我可能有小号 中号 大号三类T恤 或者我可以选择 K=5 那么我可能有 特小号 小号 中号 大号 和特大号尺寸的T恤 所以 你可能有3种 4种 或者5种T恤尺寸

这个例子的好处是 它会给我们 选择聚类数目是3 4还是5 的另一种方法 

具体来说 你要做的是 从T恤生意的角度 来思考这个事情，从你需要卖出的T恤的尺寸数量来决定你的聚类数量。

总结一下 大部分时候  聚类数目仍然是通过 手动 人工输入或我们的洞察力来决定 一种可以尝试的方法是 使用肘部法则 但是我不会总是 期望它能表现得好 我想选择聚类数目的更好方法是 去问一下你运行K-均值聚类 是为了什么目的？ 然后想一想 聚类的数目是多少才适合你运行K-均值聚类的后续目的

### Review