七、Regularization(正则化)
===

## Solving the Problem of Overfitting(过拟合问题)

## 50、The Problem of Overfitting(过拟合问题)

什么是过度拟合，我们使用用线性回归来预测房价的例子，我们通过建立以住房面积为自变量的函数来预测房价。

![50.1](http://a3.qpic.cn/psb?/V12umJF70r2BEK/zgkhOdHZ8qFUV6O40j7pRi48oVZ*eWfWPPoGtufOerM!/b/dCIBAAAAAAAA&ek=1&kp=1&pt=0&bo=YgJUAQAAAAARFxU!&tl=3&vuin=904260897&tm=1535547600&sce=60-2-2&rf=viewer_4)

第一张图使用一次函数来拟合房价数据，但随着房子尺寸的增加价格趋于稳定，导致没有很好地拟合训练数据，我们称这个问题为**欠拟合(underfitting)**，另一个说法是这个算法具有**高偏差(high bias)**

偏差(bias)这个词是过去传下来的一个专业名词，它的意思是，如果拟合一条直线就好像算法有一个很强的偏见或者说非常大的偏差，认为房子价格与面积线性相关，而罔顾(despite)数据的不符，罔顾证据的不符，先入为主地拟合一条直线，最终导致拟合数据效果很差。

第二张图，加入一个二阶项，用二次函数来拟合数据集，这个拟合的效果很好。

第三张图，一个极端情况，如果我们拟合一个四阶多项式，我们有5个参数，$\theta_0 到\theta_4$，这样我们可以通过全部五个样本拟合一条曲线，似乎是得到了一个很好的曲线，因为它通过了所有的数据点，但这是一条扭曲的曲线，它不停上下波动，我们并不认为它是个很好的预测房价的模型。这个问题我们就称之为**过度拟合**。另一个说法是这个算法具有高方差

概括地说，过度拟合问题将会在变量过多的时候出现，这时训练出的假设可以很好地拟合训练集，你的代价函数可能很小，但是你可能会得到一条如图的曲线，因为它千方百计地拟合训练集，导致它无法泛化(generalize)到新的样本中，无法预测新样本的价格。  
这里的泛化指的是一个假设模型应用到新样本的能力。

同样的问题也会出现在逻辑回归中。

![50.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/ESg3hLB7glbVUUMzuDyLksHWtfN.y8j4gfs0cClCRg8!/b/dCEBAAAAAAAA&bo=ZAJWAQAAAAARFxE!&rf=viewer_4)

和线性回归类似，图一是一个具有高偏差的欠拟合模型。

图二则很好地拟合了数据。

图三是种极端情况，也是一个过拟合例子。

如何识别过拟合和欠拟合的情况？

如何我们有很多的特征变量而却只有很少的训练数据，就会出现过度拟合的问题。

![50.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/n9iVbiJZOpJQMhPX3aCghMp3*Fbi42QGKBUwtJsHngs!/b/dCIBAAAAAAAA&bo=UwJOAQAAAAARFz4!&rf=viewer_4)

解决方法：

1、 减少选取变量的数量
- 人工检查变量清单，并决定哪些变量更为重要，哪些特征变量应该保留
- 模型选择法，可以自动选择哪些特征变量保留  
- 
2、正则化(Regularization)  
- 保留所有的特征变量，但是减少量级或者参数$\theta_j$的大小
- 当我们有很多特征变量时效果很好，每个变量都会对预测$y$值有所帮助






## 51、Cost Function(代价函数)

因为第二个函数的阶数过高，我们不妨在函数中加入惩罚项(penalizing)，使得$\theta_3$$\theta_4$都非常小，我们的目标时最小化 其均方误差代价函数，我们给这个函数加上$1000·\theta^2_3+1000·\theta^2_4$ 

![51.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/Z37GTvv4*Z1gr5t7kBjZHFGjlykgng01FAXRvlGzHq4!/b/dCEBAAAAAAAA&bo=WgJXAQAAAAARFy4!&rf=viewer_4)

当我们最小化这个新函数，我们需要使$\theta_3$$\theta_4$尽量接近0，就像我们直接去掉了这两项


这样正则化以后，参数值较小，这意味着一个更简单的假设模型。


这里有个例子，比如房屋价格的预测问题。我们有100个特征变量和参数$\theta$,但不知道哪些$\theta$是高阶项。

![51.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/BvZxogNfwzX1TRNJJ.RWOVq384NxF*UeSqa**6oN97Y!/b/dCIBAAAAAAAA&bo=QwJbAQAAAAARFzs!&rf=viewer_4)

我们要做的就是修改这个代价函数$J(\theta)$来缩小所有的参数，我们在这个代价函数的后面添加一个新的项形成正则化代价函数：

$$J(\theta) = \frac{1}{2m} \left[  \sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2 + \lambda \sum^n_{i=1}\theta^2_j \right]$$

![51.4](http://m.qpic.cn/psb?/V12umJF70r2BEK/hrIgKegx4ip.RKmwLdsx*AHwsgDay7KNRPqq2*IVWyc!/b/dCIBAAAAAAAA&bo=WwJjAQAAAAARFxs!&rf=viewer_4)

$\lambda$被称为正则化参数,$\lambda$的作用是控制两个不同目标之间的取舍,即更好的去拟合训练集的目标和将参数控制的更小的目标,从而保持假设模型的相对简单,避免出现过拟合的情况.

第一个目标是与目标函数的第一项有关,就是我们想训练，想更好地拟合数据,更好地拟合训练集。

第二个目标是我们要保持参数尽量地小,与目标函数的第二项有关,与正则化目标有关,与正则化项有关。

在正则化的线性回归中,如果正则化参数$\lambda$被设置地太大的话,结果就是我们对这些参数$\theta$的惩罚太大,那么最后所有这些参数都会趋近于0,就相当于把假设函数的全部项都忽略掉了,相当于说房屋的价格直接等于$\theta_0$,相对于说用一条水平直线来拟合数据,这就是一个欠拟合的例子.

![51.5](http://m.qpic.cn/psb?/V12umJF70r2BEK/BX5yl4y7ZjoOvdW25vNqJT9G4ebIFRqJ*nc6*edwptw!/b/dCEBAAAAAAAA&bo=ZQJRAQAAAAARBwc!&rf=viewer_4)

为了让正则化起到应有的效果,我们应该选择一个合适的$\lambda$








## 52、Regularized Linear Regression(线性回归的正则化)

对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。

正则化线性回归的代价函数为：

$$J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{\left[ {{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})}^{2}}+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2}}\right] }$$

我们要找到一个参数$\theta$来最小化代价函数

如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对$\theta_0$进行正则化，所以要对$\theta_0$做区别对待，因此梯度下降算法将分两种情形：

$Repeat$ $until$ $convergence${

​${\theta_0}:={\theta_0}-α\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})$

​${\theta_j}:={\theta_j}-α\left[\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}\right]$

​ $for$ $j=1,2,...n$

​ }

对上面的算法中$j=1,2,...,n$ 时的更新式子进行调整可得：

${\theta_j}:={\theta_j}(1-α\frac{\lambda }{m})-α\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \ right)}}​$ 

![52.1](http://a3.qpic.cn/psb?/V12umJF70r2BEK/giwoKKSc3qkx.7s.3qtTLdwWqohFsqemKIkCDUx3NcI!/b/dCIBAAAAAAAA&ek=1&kp=1&pt=0&bo=RwPTAQAAAAARF7Y!&tl=3&vuin=904260897&tm=1535598000&sce=60-2-2&rf=viewer_4)

在我们进行正则化线性回归时，我们要做的就是每次迭代时d偶将$\theta_j$乘一个比1略小的数，可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$\theta$值减少了一个额外的值。

![52.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/fKns3*.BNKqbpUJ*95PwbYOTmaHRXtkW7uk67WiVGhY!/b/dCIBAAAAAAAA&bo=UwPRAQAAAAARF6A!&rf=viewer_4)

我们同样也可以利用正规方程来求解正则化线性回归模型，

我们建立一个m*(n+1)维的设计矩阵$X$,它的每一行都代表一个单独的训练样本,然后建立一个m维向量$y$，它包含了训练集里所有标签。

我们想用正则化来得到想要的最小值,方法是将对各个参数的偏导数设为0，然后进行一些数学推导,你就能得到下面的式子并使得代价函数最小。

最后对原正规方程修改后的带有正则化的正规方程如下所示：

$$\theta = \begin{pmatrix}
    X^TX+\lambda\begin{bmatrix}
        0 \ \ \ \ \\
        \ 1 \ \ \ \\
        \ \ 1 \ \ \\
        \ \ \ \dots \\
        \ \ \ \ \ 1 \\
    \end{bmatrix}
\end{pmatrix}^{-1} X^Ty$$
图中的矩阵尺寸为 $(n+1)*(n+1)$。


## 53、Regularized Logistic Regression(Logistic回归的正则化)

针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：我们首先学习了使用梯度下降法来优化代价函数$J\left( \theta \right)$，接下来学习了更高级的优化算法，这些高级优化算法需要你自己设计代价函数$J\left( \theta \right)$。

![53.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/B55PDl2SrpYj*PBCxEK.SwQvHWnKVxaEO91jSoMl2FE!/b/dCEBAAAAAAAA&bo=lAJxAQAAAAARB9Y!&rf=viewer_4)

自己计算导数同样对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数：

$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}+\frac{\lambda }{2m}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$

这样做的话,产生的效果就是,即使当你的拟合阶数很高且参数很多,只要添加了这个正则化项,保持参数较小，你仍然可以得到这样一条不错的决策边界.正则化就这样帮助你避免了过拟合的现象。

![53.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/o.q58YJ9k2MTh1.nUl2tcX0N9UyT47.aPOfrijqMjVU!/b/dCIBAAAAAAAA&bo=fgJpAQAAAAARFzQ!&rf=viewer_4)

要最小化该代价函数，通过求导，得出梯度下降算法(同样对$\theta_0$单独处理)为：

$Repeat$ $until$ $convergence${

​ ${\theta_0}:={\theta_0}-α\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})$

​ ${\theta_j}:={\theta_j}-α[\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]$

​ $for$ $j=1,2,...n$

​ }

注：正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，，但是知道 ${h_\theta}\left( x \right)=g\left( {\theta^T}X \right) = \frac{1}{1+e^{-\theta^Tx}}$与线性回归的$h_\theta$不同，所以与线性回归不同。 Octave 中，我们依旧可以用 fminuc 函数来求解代价函数最小化的参数，值得注意的是参数${\theta_{0}}$的更新规则与其他情况不同。 注意：



${\theta_{0}}$不参与其中的任何一个正则化。

接下来是更高级的优化算法

![53.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/RZaWuwvu5d6OAS6m7ntrAc3vDvzzrPJp1*ylyatwXac!/b/dCIBAAAAAAAA&bo=QgSNAgAAAAARF.k!&rf=viewer_4)

目前大家对机器学习算法可能还只是略懂，但是一旦你精通了线性回归、高级优化算法和正则化技术，坦率地说，你对机器学习的理解可能已经比许多工程师深入了。现在，你已经有了丰富的机器学习知识，目测比那些硅谷工程师还厉害，或者用机器学习算法来做产品。

接下来的课程中，我们将学习一个非常强大的非线性分类器，无论是线性回归问题，还是逻辑回归问题，都可以构造多项式来解决。你将逐渐发现还有更强大的非线性分类器，可以用来解决多项式回归问题。我们接下来将将学会，比现在解决问题的方法强大N倍的学习算法。





### Review





