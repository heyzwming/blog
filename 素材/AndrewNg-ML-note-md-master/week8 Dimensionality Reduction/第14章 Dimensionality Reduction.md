十四、Dimensionality Reduction(降维)
===
## Motivation(目标)
---
## 14.1、 Motivation Ⅰ:Data Compression(目标Ⅰ：介绍)

在这节中，我们介绍 第二种无监督学习问题 它叫降维 (dimensionality reduction) 

我们希望使用降维的原因有以下几个：一、数据压缩。数据压缩不仅通过，压缩数据使得数据占用更少的计算机内存和硬盘空间，它还能给算法提速

首先 我们来介绍 什么是降维 

举一个例子 假如我们有一个有很多很多很多特征变量的数据集，我在这里只画了其中两个，这两个特征变量描述了一个某个物体的长度，特征变量x1以厘米为单位 另一个特征变量x2是它以英寸为单位的长度 

所以这是一个 非常冗余的数据，两个特征变量x1和x2都是测量到的长度 或许我们应该把 这个数据降到一维 这样一来就只用一个长度的数据特征变量来描述这个物体的长度。

在实际上得到这样冗余的特征变量并不难 所以如果我们可以把数据从二维降到一维就可以减少冗余 

再举一个例子 可能看起来更实际一些 

![飞行员原始图]()

这些年来 我和直升机飞行员们在一起工作了很多年，那么如果你要测量、研究或者给这些不同的飞行员做测试的话，你就可能会有一个特征x1，它可能是这些直升机飞行员的技术，x2可能是飞行员的工作愉快程度，这是指他们有多享受飞行，可能这两个特征变量的相关程度非常高，但是你真正关心的可能是斜向上方向的，一个不同的，用来真正测量飞行员能力的特征，就取名为aptitude(能力)趴。但是如果你的两个特征高度相关 你可能真的需要降低维度 

让我们再详细讲讲，我们将数据从二维降到一维到底意味着什么,让我给这些样本,涂上不同的颜色 

![图上不同颜色后的飞行员]()

在这个例子中 降低维度的意思是 我希望找到 这样一条线,一条基本所有点都落在这个方向上的一条线，然后把所有的数据映射到这条线上，这样做之后我就可以直接测量这条线上每个样本的位置,我想把这个新特征叫做 z1 

要确定这条线上的位置 我只需要一个数字能够表示这条绿线上每一个点的位置,这个意思就是说在之前如果我有一个样本 x(1) 比如这是我的 第一个样本x(1)所以为了表示 原本的x(1),我需要一个二维数字 或者一个二维特征向量 但是现在我可以只用 z(1)  来表示我的第一个样本 它就是一个实数 类似地 如果 x(2)  是我这里的第二个样本 那么在之前 需要两个数字来表示它 但是如果我计算这条线上那个 黑色×的投影 那么现在我只需要一个实数 也就是 z(2) 来表示这条线上 这个点 z(2) 的位置 这个点 z(2) 的位置以此类推到我的 m 个样本上

总结一下 如果我们允许 

通过把所有原始的样本 映射到这条绿线上 来近似原始的数据集 那么我就 只需要一个数字 我只需要一个实数 来确定这条线上 一个点的位置 这样一来 在把所有训练样本 映射到这条绿线上之后 我就能只用一个数字来表示 每个训练样本的位置 

所以这是一个对原始训练样本的近似 这是因为我把所有训练样本 映射到这条线上了 但是现在 我只需要给 每个样本保留一个数 

这样就减少了一半的内存需求 或者硬盘需求 或者是你用来储存数据的东西 

可能更令人感兴趣 也更重要的是 我们之后会看到 我们在之后的视频要讲的 是数据压缩还会 让我们的学习算法 运行地更快 而那实际上可能是 在数据压缩的应用里 比减少硬盘存储空间的需求 这一应用 更令人感兴趣的 

在上一页幻灯片中 我们展示了一个 把数据从 2D 降到 1D 的例子 在这页幻灯片中 我要展示另一个 把数据从三维 3D 降到二维 2D 的例子 

顺便说一下 在更典型的维数约减例子中 我们可能有1000维 1000D 的数据 我们可能想降低到100维 也就是 100D 但是因为我在幻灯片上 能画的图是有限制的 所以我要用的例子是 3D 到 2D 或者 2D 到 1D 的 

所以我们有一个图上这样的数据集 我有一个样本 x(i) 的集合 x(i) 是一个三维实数的点 所以 我的样本是三维的 从这个幻灯片上 可能难以看出这点 但是我一会儿会 展示一个 3D 点云 虽然可能从这里很难看出来 但所有这些数据 可能都差不多 在这样的一个平面上 

那么维数约减在这儿怎么用呢 那么维数约减的作用 就是把所有数据 投影到一个二维平面上 投影到一个二维平面上 所以在这里我做的是 对所有的数据 进行投影 使得它们落在这个平面上 

现在 最后 为了表示 一个点在平面上的位置 我们需要两个数 对吧？ 我们需要表示这个点 沿着这个坐标轴的位置 然后还要表示这个点 沿着另一个坐标轴的位置 所以 我们需要两个数 来表示平面上一个点的位置 这两个数可能叫做 z1 和 z2 这个意思是 我们现在可以用 两个数来表示 每一个训练样本 就是写在这里的 z1 和 z2 

所以我们的数据可以用向量 z 表示 它是一个二维实数 

这些下标 z下标1 z下标2 他们的意思是 这个向量z 是二维向量 z1 z2 所以如果我的 某个样本是 z(i) 它就是一个 二维向量 z(i)1 z(i)2 

在上一个幻灯片中 当我把数据的维度 降到一维时 我只有 z1 对吧？ 在之前的幻灯片中 是 z1 下标是1 但是这里我的数据是二维的 所以我有 z1 和 z2 作为数据的两个元素 

现在 让我们来确认一下 都能理解这些图片的意义 现在让我们用 3D 绘图 来重现一样的这三幅图 我们走的过程是这样的 左边是原始数据集 中间是投影到 2D 的数据集 右边是以 z1 和 z2 为坐标轴的 2D 数据集 我们来更详细地看一下 左边这个 这是我的原始数据集 所以我从一个这样的 3D 点云开始 3D 点云开始 它的坐标轴是 x1 x2 x3 所以这是一个 3D 的点云 但是大部分数据 差不多可能都落在某个 2D 平面上 或者说距离某个 2D 平面不远 

所以 我们可以做的是 取这些数据 这是我中间的图片 我要把它们投影到 2D 所以 我投影这些数据 使得它们全部落在这个 2D 平面上 你可以看到所有的数据落在一个平面上 因为我们把所有的东西 都投影到一个平面上了 所以这个意思是 我们现在只需要 两个数 z1 和 z2 来表示点在平面上的位置 

所以这就是 把数据从三维 降到二维的过程 降到二维的过程 这就是维数约减 以及如何使用它来压缩数据 

我们之后会看到 它还会让我们的某些算法 运行得更快 我们在后面的课中继续介绍 

## 14.2、 Motivation Ⅱ:Visualization(目标Ⅱ：可视化)

在上节课的视频中 我们讲到一种通过数据降维 来进行数据压缩的方法 在本节课的视频中 我将会讲到第二种数据降维的应用 那就是可视化数据 对于大多数的机器学习 应用 它真的可以帮助 我们来开发高效的学习 算法 前提是我们能更好地理解数据 如果有某种 将数据可视化更好的办法 那就是维度降低 通常来说 它都是一个有用的工具 让我们从一个例子开始 

假如我们已经收集了大量的 统计数据集 有关全世界不同国家的 或许第一个特征x1 是国家的  或者说是国内生产总值 x2是一个百分比 每人占有的GDP x3 人类发展指数 x4 预期寿命 x5 x6 等其它特征 我们也许会有大量的数据集 像这里这样的数据 对于每个国家可能有50个特征 我们有这样的众多国家的数据集 

那么有没有办法 使得我们能更好地来理解数据 这里我给出了一张有数字的表格 你怎样将这些数据可视化 如果有50个特征 绘制一幅50维度的图 

是异常困难的 那有没有观察数据的好办法呢 

使用降维的方法 那么应该怎么做呢 我们使用特征向量x(i) 来表示每个国家 x(i)有着50个维度 举个例子 加拿大这个国家 其国家特征用这50个数字来代表 加拿大 我们可以提出一种不同的 特征表示方法 使用一个二维的向量z来代替x 

在这种情况下 我们可以使用一对数字 z1和z2 从某种程度来说 这两个数总结了50个数 也许 我们可以 使用这两个数来绘制出这些国家的二维图 使用这样的方法尝试去 理解二维空间下 不同国家在不同特征的差异 更容易 所以 这里你能做的是 将数据降维 从50维度 的数据 降维到2维度 这样你就可以绘制出 2D的图像了 当你这么做时 你会发现如果 你仔细观察降维算法的输出结果 它通常不能赋予 你想要的这些二维新特征一个物理含义 你应该能想来 这经常取决于我们计算出的特征含义 

但如果你绘制出这些特征 你也许会发现 在这里 每个国家 用一个点z(i)表示 z(i)是一个二维数据 这些每个点 都是如此 通过使用数字代表了一个国家 这里是z1 这个是z2 使用这两个数字组成的坐标代表国家 所以 你或许会发现 例如 那条水平轴  即z1轴 

大致对应了 国家总面积 或者一个国家的总体经济活动情况 整体  一个国家的整体经济规模 然而纵轴的 数据或许对应着 人均 GDP 或是人均幸福感 或是人均经济活动 你也许能发现 有了这50个特征 到最后主要是这2个 维度的特征来进行表示 在这里你或许能有一个国家 像美国 有着相当大的  你应该也能想来 这么大的总 GDP 以及相对的高人均 GDP 然而你也能看到 像新加坡这样的国家 事实上 新加坡也有很高的人均 GDP 但由于新加坡是一个非常 小的国家 

国家整体经济规模 

比美国要小得多 

同时在这里 你能看到有些国家 

人们并不富裕 或许人均寿命较低 

卫生保健较差 经济较为不成熟 

这也就是为什么 国家越小 那个点对应的 国家就会有集会 就会有很多的经济活动 然而在这些国家 可能幸福感并不是很高 所以你或许能发现 通过观察z1和z2轴 你可以最为简洁地 捕捉到两个维度的变量 变化情况 

在这些不同的国家中 

比如说国家的整体经济 活动映射着 国家整体经济的规模 同时 也映射了个人的 幸福感 我们可以通过测算人均 

GDP 人均卫生保健等其它类似的指标进行测算 

那么你如何 使用降维的方法 把数据从 50维或者其它的维度 降到2个 或者3个维度呢 为了你可以将数据绘出并更好地理解数据 

在下一节视频中 我们将会开始开发一种特别的算法 简称PCA 或者主成分分析 这个算法允许我们进行数据可视化 同时可以进行早先我们提到的一些 

有关数据压缩方面的应用 

## Principal Component Analysis(主成分分析)
---
## 14.3、 Principal Component Analysis Problem Formulation(主成分分析问题规划)

Help us translate!

对于降维问题来说 目前 最流行 最常用的算法是 主成分分析法 (Principal Componet Analysis, PCA） 

在这段视频中 我想首先开始讨论 PCA问题的公式描述 也就是说 我们用公式准确地精确地描述 我们想让 PCA 来做什么 假设 我们有这样的一个数据集 这个数据集含有 二维实数空间内的样本X 假设我想 对数据进行降维 从二维降到一维 

也就是说 我想找到 一条直线 将数据投影到这条直线上 那怎么找到一条好的直线来投影这些数据呢？ 

这样的一条直线也许是个不错的选择 你认为 这是一个不错的选择的原因是 如果你观察 投影到直线上的点的位置 我将这个点 投影到直线上 得到这个点 这点被投影到这里 这里 这里 以及这里 我们发现 每个点到它们对应的 投影到直线上的点之间的距离非常小 

也就是说 这些蓝色的 线段非常的短 

所以 正式的说 PCA 所做的就是 寻找一个低维的面 在这个例子中 其实是一条直线 数据投射在上面 使得 这些蓝色小线段的平方和 达到最小值 这些蓝色线段的长度 时常被叫做 投影误差 所以 PCA 所做的就是寻找 一个投影平面 对数据进行投影 使得这个能够最小化 另外 在应用PCA之前 通常的做法是 先进行均值归一化和 特征规范化 使得 特征 x1 和 x2 均值为0 数值在可比较的范围之内 在这个例子里 我已经这么做了 但是 在后面 我还将回过来讨论更多有关 PCA背景下的特征规范化和均值归一化问题 

回到这个例子 对比 我刚画好的红线 这是另一条对数据进行投影的直线 这条品红色的线 如你所见 你知道 用这条品红色直线 来投影我的数据 是一个非常糟糕的方向 对吧？ 所以 如果我将数据 投影到这条品红色的直线上 像我们刚才做的那样 

那么投影误差 就是这些蓝色的线段 将会很大 所以 这些点将会 移动很长一段距离 才能投影到 才能 投影到这条品红色直线上 因此 这就是为什么 PCA 主成分分析法会选择 红色的这条直线 而不是品红色的这条直线 

我们正式一点地写出 PCA 问题 PCA 的目标是 如果我们将数据从二维 降到一维的话 我们将试着寻找 一个向量 向量 u(i) 属于 n 维空间中的向量 在这个例子中是二维的 我们将寻找一个对数据进行投影的方向 使得投影误差能够最小 在这个例子里 我们希望 PCA 寻找到 这个向量 我将它叫做 u(1) 所以 当我把数据投影到 

我定义的这条直线 通过延长这个向量得到的直线 最后我得到非常小的 重建误差 看上去是这样的 另外 我应该指出的是 无论PCA 给出的是这个 u(1) 还是负的 u(1) 都没关系 如果它给出的是正的向量 在这个方向上 这没问题 如果给出的是相反的向量 在相反的方向上 也就是 -u(1) 用蓝色画出来 无论给的是正的 还是负的 u(1)  都没关系 因为 这两个方向都定义了 相同的红色直线 也就是我将投影的方向 这就是将 二维数据降到一维的例子 更一般的情况是 我们有 n 维的数据 想降到 k 维 

在这种情况下 我们不仅仅只寻找单个的向量 来对数据进行投影 我们要找到 k 个方向 来对数据进行投影 从而最小化投影误差 这是一个例子 如果我有一些三维数据点 比如说像这样的 我想要做的是 

是寻找两个向量 我将这些向量叫做... 我们用红线画出来 我要寻找两个向量 从原点延伸出来 这是 u(1)  这是第二个向量 u(2)  这两个向量一起 定义了一个平面 或者说 定义了一个二维面 

就像这样 二维平面 我将把数据投影到上面 对于你们其中 熟悉线性代数的人来说 对于你们其中真的 精通线性代数的人来说 对这个正式的定义是 我们将寻找一组向量 u(1) u(2) 也许 一直到 u(k)  我们将要做的是 将数据投影到 这 k 个向量展开的线性子空间上 

但是如果你不熟悉 线性代数 那就想成是 寻找 k 个方向 而不是只寻找一个方向 对数据进行投影 所以 寻找一个 k 维的平面 在这里是寻找二维的平面 如图所示 这里我们用 

k 个方向来定义平面中这些点的位置 这就是为什么 对于PCA 我们要寻找 k 个向量来对数据进行投影 因此 更正式一点的说 在PCA中 我们想做的就是 寻找到这种方式 对数据进行投影 进而最小化投影距离 也就是数据点和投影后的点之间的距离 在这个三维的例子里 给定一个点 我们想将这个点 投影到二维平面上 

当你完成了那个 因此投影误差就是 也就是 这点与投影到 二维平面之后的点之间的距离 因此 PCA 做的就是 寻找一条直线 或者平面 诸如此类等等 对数据进行投影 来最小化平方投影 90度的或者正交的投影误差 最后 一个我有时会被问到的问题是 PCA 和线性回归有怎么样的关系？ 因为当我解释 PCA 的时候 我有时候会以 画出这样的图 看上去有点像线性回归 

但是 事实是 PCA不是线性回归 尽管看上去有一些相似 但是它们确实是两种不同的算法 

如果我们做线性回归 

我们做的是 看左边 我们想要 在给定某个输入特征 x 的情况下 预测某个变量 y 的数值 因此 对于线性回归 我们想做的是 拟合一条直线 

来最小化 点和直线之间的平方误差 所以我们要最小化的是 这些蓝线幅值的平方 注意我画的这些 蓝色的垂直线 这是垂直距离 它是某个点 与通过假设的得到的其预测值之间的距离 与此想反 PCA要做的是 最小化这些蓝色直线的幅值 倾斜地画出来的 这实际上是最短的 直角距离 也就是点 x 跟红色直线之间的最短距离 这是一种非常不同的效果 取决于数据集 更更更一般的是 当你做 线性回归的时候 有一个 特别的变量 y 是我们将要预测的 线性回归所要做的就是 用 x 的所有的值来 预测 y 然而在 PCA 中 没有这么一个特别的或者 特殊的变量 y 是我们要预测的 我们所拥有的是 特征x1 x2 等 一直到xn 所有的这些特征都是被同样地对待 因此 它们中没有一个是特殊的 

最后一个例子 如果我有三维数据 我要将这些数据 从三维降到二维 我就要找到两个方向 

也就是 u(1) 和 u(2) 将数据投影到它们上面 然后我得到的是 我有3个特征 x1 x2 x3 所有的这些都是被同样地对待 这些都是被均等地对待 没有特殊的变量 y 需要被预测 

因此 PCA 不是线性回归 尽管有一定程度的相似性 使得它们看上去是有关联的 但它们实际上是非常不同的算法 因此 希望你们能理解 PCA 是做什么的 它是寻找到一个低维的平面 对数据进行投影 以便 最小化投影误差的平方 最小化每个点 与投影后的对应点之间的距离的平方值 

在下一段视频中 我们将开始讨论 如何真正地找到这个低维平面 来对数据进行投影 

## 14.4、 Principal Component Analysis Algorithm(主成分分析算法)

Help us translate!

在这段视频中 我想介绍一下 主成成分分析(PCA)的算法 

听完这段视频 你就应该知道 PCA 的实现过程 并且应用 PCA 来给你的数据降维了 在使用 PCA 之前 我们通常会有一个数据预处理的过程 拿到某组有 m 个无标签样本的训练集 一般先进行均值归一化 (mean normalization) 这一步很重要 

然后 还可以进行特征缩放 (feature scaling) 这根据你的数据而定 

这跟我们之前 在监督学习中提到的 均值归一和特征缩放是一样的 实际上 它们是完全一样的步骤 只不过现在我们针对的 

是一系列无标签的数据 x(1) 到 x(m) 因此 对于均值归一 我们首先应该计算出 每个特征的均值 μ 然后我们用 x - μ 来替换掉 x 这样就使得 所有特征的均值为0 

然后 由于不同特征的取值范围都很不一样 比如说  如果 x1 表示房子的面积 x2 表示房屋的卧室数量 这里沿用我们之前的例子 然后我们可以把每个特征 进行缩放 使其处于同一可比的范围内 同样地 跟之前的监督学习类似 我们可以用 x(i)j 减去平均值 μj 

除以 sj 来替换掉第 j 个特征 x(i)j 这里的 sj 表示特征 j 的某个量度范围 因此它可以表示最大值减最小值 或者更普遍地 它可以表示特征 j 的标准差 进行完以上这些数据预处理后 接下来就正式进入 PCA 的算法部分 

在之前的视频中 我们已经知道了 PCA 的原理 PCA 是在试图找到一个 低维的子空间 然后把原数据投影到子空间上 并且最小化平方投影误差的值 或者说  投影误差的平方和 也就是 这些蓝色线段 长度的平方和 因此 我们想要做的 是找到某个具体的向量 u(1) 指定这条投影线的方向 或者 在2D的情况下 我们想要找到两个向量 

u(1) 和 u(2) 来定义一个投影平面 对数据进行投影 

因此 我们再来回忆一下 降低数据的维度是什么意思 对于左边这个例子 我们的数据是 二维实数 x(i) 我们想要做的 是找到一系列 一维实数 z(i) 来表示我们的数据 

因此这就是从二维降低到一维的意思 

具体来说就是 

要把数据投影到这条红线上 我们只需要一个数 来指明点在线上的位置 我把这个数称为 

z 或者 z1 这里的 z 就是一个实数 等于是一个一维向量 所以 z1 就指的是 这个 1×1 的 z 向量的 第一个元素 

因此 我们只需要一个数 就能指定点在线上的位置了 所以 如果这个样本 是我们的样本 x(1) 

那么它可能投影到这里 如果这是我的样本 x(2) 那么它可能被投影到这里 因此这个投影点 就是 z(1) 而这个投影点 就是 z(2) 类似地 我们也可以有其他点 比如 x(3) x(4) x(5) 等等 投影到 z(1) z(2) z(3) (老师口误,译者注) 

因此 PCA 要做的事儿 就是要得到一种方法 来计算两个东西 其一是计算这些向量 

比如这里的 u(1) 这里的 u(1) u(2) 另一个问题是 怎样计算出这些 z 

对于左边这个例子 我们要把数据从二维降到一维 

对于右边这个例子 我们要把数据从三维 降到二维 从原来的 x(i) 变为现在的 z(i) 新的 z 向量是二维的 它应该是 {z1 z2] 这样的一个向量 因此 我们需要 找到某种办法 来算出这些新的变量 也就是 z1 和 z2 那么应该怎样来计算这些值呢? 实际上 这个问题 有它在数学上的推导 或者说有完整的数学证明 来解释 什么才是 u(1) u(2) z1 z2 的正确值 这个数学证明过程 是非常复杂的 同时也超出了本课程的范围 但如果你推导一遍 这个数学证明过程 你就会发现要找到 u(1) 的值 也不是一件很难的事 但是要证明 这个值就应该是正确的取值 这是别的老师的任务 不是我要讲的内容 我只简单描述一下 你要实现 PCA  要计算这些值 计算这些向量 u(1) u(2) 以及 z 向量 

所需要进行的步骤 

假如说 我们想要 把数据从 n 维 降低到 k 维 

我们首先要做的 是计算出 这个协方差矩阵 通常是用 希腊字母大写的西格玛  ∑ 来表示 

很不幸的是 这个希腊符号和 求和符号重复了 这里说的是 表示协方差矩阵的符号 而这里表示的是求和符号 希望在这部分幻灯片中 我的表达让你不要混淆 哪个是表示协方差的  ∑ 矩阵 哪个是求和符号 希望从我使用的环境中 你能看出来 计算出这个协方差矩阵后 假如我们 把它存为 Octave 中的 一个变量 叫 Sigma 我们需要做的 是计算出 Sigma 矩阵的特征向量 (eigenvectors) 

在 Octave 中 你可以使用如下命令 来实现这一功能 [U,S,V] = svd(Sigma); 

顺便说一下 svd 表示奇异值分解 (singular value decomposition)  

这是某种更高级的 奇异值分解 

这是比较高级的 线性代数的内容 你不必掌握这些 但实际上 Sigma 是一个 协方差矩阵 有很多种方法 来计算它的特征向量 如果你线性代数学得很好 或者你之前听说过 特征向量的话 那也许知道在 Octave 中 还有另一个 eig 命令 可以用来计算特征向量 实际上 svd 命令 和 eig 命令 将得到相同的结果 虽然说 svd 其实要更稳定一些 所以我一般选择用 svd 但我也有一些朋友 他们喜欢用 eig 函数 但你用 Sigma 命令 用在这里的协方差矩阵上 你会得到同样的答案 这是因为协方差均值 总满足一个数学性质 称为对称正定 (symmetric positive definite) 你不必细究 这个具体是什么意思 

你只要知道 svd 和 eig 是不同的函数 但当它们用在 协方差矩阵时 可以证明它始终是满足 

这个数学性质的 因此用两个命令的结果一样 

好了 这就是你需要了解的一点线性代数知识 如果有任何地方不清楚的话 不必在意 你只需要知道 这条在 Octave 中 你需要执行的语句就行了 如果你用除了 Octave 或者 MATLAB 之外的其他编程环境 你要做的是找到 某个可以计算 svd  即奇异值分解的 函数库文件 在主流的编程语言中 应该有不少这样的库文件 我们可以用它们 来计算出协方差矩阵的 U S V 矩阵 我再提一下 几个细节问题 这个协方差矩阵 Sigma 应该是一个 n×n 的矩阵 一种证实的办法是 如果你看定义 

这是一个 n×1 的向量 这一项有一个转置 因此是一个 1×n 的向量 两个向量相乘 得到的结果 自然是 n×n的矩阵 

这是 n×1 的转置 1×n 所以这是一个 n×n 的矩阵 然后把所有这些加起来 当然还是 n×n 矩阵 

然后 svd 将输出三个矩阵 分别是 U S V 你真正需要的是 U 矩阵 

U 矩阵也是一个 n×n 矩阵 

如果我们看 U 矩阵的列 实际上 U 矩阵的 列元素 

就是我们 需要的 u(1) u(2) 等等 

所以 U 矩阵是 n×n 的 

如果我们想 将数据的维度从 n 降低到 k 的话 我们只需要提取前 k 列向量 

这样我们就得到了 u(1) 到 u(k) 也就是我们用来投影数据的  k 个方向 剩下的步骤就很简单了 通过这个 svd  我们得到了矩阵 U 我们把它的列向量 叫做 u(1) 到 u(n) 

然后我们继续 把剩下的步骤讲完 通过这个 svd 过程 我们可以得到 矩阵 U S V 我们取出 U 矩阵的 前 k 列 得到一个新的矩阵 u(1) 到 u(k) 

接下来我们要做的事是 我们需要找到一个办法 对于原始数据集 x x 是一个 n 维实数 然后我们要找到一个 低维的表达 z z 是 k 维实数 所以方法是 我们把 U 矩阵的前 k 列取出来 

我们构建这样一个矩阵 

把 u(1) u(2) 

一直到 u(k) 并列地合起来 其实就是取出 这个 U 矩阵的 前 k 列元素 

因此这就是一个 n × k 维的矩阵 n × k 维的矩阵 我给这个矩阵起个名字 我把这个矩阵 叫做 U 下标 reduce 表示 U 矩阵约减后的版本 我将用它来约减我的数据 

然后 计算 z 的方法是 z 等于这个 Ureduce 矩阵的转置乘以 x 或者也可以这样写 这里的转置可以换一种写法 对 U 矩阵求转置 实际上就会得到 这样一个行矩阵 从 u(1) 转置一直到 u(k) 转置 

然后用它乘以 x 这样我就得到了 z 矩阵的表达 我们来标出矩阵的维度 

这个矩阵的维度 应该是 k × n 这里 x 的维度 应该是 n × 1 因此这两个相乘 维度应该是 k × 1 因此 z 是 k 维的 向量 正好也就是 我们所希望的 我们所希望的 当然 这里所说的 x 可以是训练集中的样本 也可以是 交叉验证集中的样本 也可以是测试集样本 比如 如果我想处理第 i 个训练样本 那么我可以把这个写成 

x(i) 这里也是 x(i) 降维得到的就是 z(i) 总结一下 这就是 PCA 的全过程 

首先进行均值归一化 保证所有的特征量都是均值为0的 然后可以选择进行特征缩放 如果不同特征量的范围跨度很大的话 你确实需要进行特征缩放这一步 在以上的预处理之后 我们计算出这个协方差 Sigma 矩阵 就像这样 顺便说一下 如果你的数据 是像这样的一个矩阵给出的 也就是说你的数据是一行一行给出的 比如数据是一个大 X 矩阵 每一组训练样本用一行来表示 从 x(1) 转置 一直到 x(m) 转置 

那么这个协方差矩阵 就能写成一个向量化的表示 

你可以在 Octave 中实现 在 Octave中 你可以 执行 Sigma = (1/m) * X' * X; 执行 Sigma = (1/m) * X' * X; X 就是上面这个矩阵 你只要执行这个简单的语句 这用向量化的表达 计算出了 Sigma 矩阵 

今天我不会在这里证明 这个计算公式的正确性 当然如果你愿意 你也可以自己推导一下 或者在 Octave 中测试一下 确保这两个式子其实是一样的 或者你动手自己算一算吧 

随便你怎么证明 反正这个向量表达是正确的 

然后我们可以应用 svd 函数 来计算出 U S V 矩阵 然后 我们取出 U 矩阵的 前 k 列元素 组成新的 Ureduce 矩阵 最后这个式子 给出了我们从 原来的特征 x  变成降维后的 z 的过程 另外 跟 k均值算法类似 如果你使用 PCA 的话 你的 x 应该是 n 维实数 所以 没有 x0 = 1 这一项 好了 这就是 PCA 算法 

有一件事儿我没做 u(1) u(2) 等等 通过将数据 投影到 k 维的子平面上 确实使得 投影误差的平方和为最小值 

我并没有证明这一点 已经超出了这门课的范围 幸运的是 PCA 算法 能够用不多的几行代码 就能实现 如果你在 Octave 或者 MATLAB 中 自己实现一下的话 你就已经获得了一种 非常有效的维度约减的算法 

这就是 PCA 算法 

我没有证明 u(1) u(2) 等等 以及 z 等等 这些我们在算法中的选择 确实能够使得 平方投影误差最小化 对吧? 还记得吗? 我们之前说过的 PCA 的目标就是 尽量找到一个平面 或者一条线 来对数据进行投影 这个平面或线应该最小化平方投影误差 我并没有证明这一点 这个问题的数学证明 已经超出了这门课的范围 但很幸运的一点是 PCA 算法可以在 Octave 中 用短短几行代码执行出来 如果你自己执行一遍 它一定会成功运行的 而且效果会很好 如果你亲自实现这个算法的话 你就得到了这个非常有用的降维算法 它确实能够很好地最小化 平方投影误差的值 
## Applying PCA(应用PCA)
---
## 14.5、 Reconstruction from Compressed Representation(压缩重现)

在前面中，我们介绍了 PCA (主成分分析) 作为压缩数据的算法 你会发现 它能将高达一千维度 的数据压缩到 只有一百个维度 或者将三维数据 压缩到两个维度的情况 

如果有一个这样的 压缩算法 那么也应该有一种方法可以把压缩过的数据近似地回到原始高维度的数据。

假设有一个已经被压缩过的 $z^{(i)}$ 它有100个维度 怎样使它回到其最初的表示 $x${(i)}$ 也就是压缩前的1000维的数据呢？ 

![14.5.1](http://a1.qpic.cn/psb?/V12umJF70r2BEK/GymsJLjKeXq8L.cUbB4eVE7TaM17*tJM6UMVtna7A5g!/b/dPQAAAAAAAAA&ek=1&kp=1&pt=0&bo=nwP3AQAAAAARF0o!&tl=3&vuin=904260897&tm=1536224400&sce=60-2-2&rf=viewer_4)

压缩重现问题，具体来说就是给出一个由二维实数点压缩来的一维实数点z，我们要让z重新变成原来的二维实数点x？

我们知道$z = U^T_{reduce}x$ 如果想得到相反的情形 方程应这样变化 $X_{approx} = U_{reduce}*z^{(1)}$

在这里 $U_{reduce}$ 是一个n×k矩阵 $z$就是一个k×1维向量 将它们相乘得到的就是n×1维 所以说 $X_approx$ 是一个n维向量.同时 根据 PCA 的意图 投影的平方误差不能很大,也就是说 $X_approx$ 将会与最开始用来导出z的原始x很接近,用图表示出来就是右图这样的情况，在这一过程后 我们可以看到 这些点都到绿线上去了，对比左图这样压缩数据时的图就可以明白两者的差别了，这已经是 与原始数据非常近似了。

用低维度的特征数据$z$压缩重现回到未被压缩的特征数据 我们找到一个 与原始数据x近似的$X_apporx$我们也称这一过程为原始数据的重构 ( reconstruction ) 我们可以在需要从压缩过的数据重构出原始数据x时使用这种方法.

## 14.6、 Choosing the Number of Principal Components(主成分数量选择)

在 PCA 算法中我们把n维特征变量降维到k维特征变量,这个数字k是PCA算法的一个参数,也被称作主成分的数量或者说是我们保留的主成分的数量

为了选择参数k 也就是要选择主成分的数量 这里有几个有用的概念 

![14.6.1](http://a2.qpic.cn/psb?/V12umJF70r2BEK/k8tU.3CxPRkYwh90APMdVtboFpyUPnXTHK2sovFrip8!/b/dIUBAAAAAAAA&ek=1&kp=1&pt=0&bo=DgebAwAAAAARF7E!&tl=3&vuin=904260897&tm=1536231600&sce=60-2-2&rf=viewer_4)

1、“平均平方映射误差”(Average Squared Projection Error)。PCA所做的就是尽量最小化这个平均平方误差，即最小化原始数据x和其在低维表面上的映射点X_approx(i)之间的距离的平方

$$\frac{1}{m}\sum_{i=1}^m \begin{Vmatrix}
    x^{(i)} - X_approx^{(i)}
\end{Vmatrix}^2$$

2、数据的总变差 (Total Variation)。它是这些样本$x^{(i)}$的长度的平方的均值,因此数据的总变差就是我这些训练集中每个训练样本长度的平均值,它的意思是 “平均来看我的训练样本距离原点多远？”

$$\frac{1}{m} \sum^m_{i=1} \begin{Vmatrix}
    x^{(i)}
\end{Vmatrix}^2$$

当我们去选择k值的时候,一个常见的经验法则是: 选择能够使得它们之间的比例小于等于0.01的最小的k值 

换言之 一个非常常用的选择k值的方法是: 我们希望平均平方映射误差 就是x和其映射值之间的平均距离除以数据的总变差,以此来体现数据的变化有多大 

大部分人在考虑选择k的方法时 不是直接选择k值而是这个数应该是多少 它应该是0.01 还是其它的数 如果是0.01 换言之 用PCA的语言说就是 保留了99%的差异性 

数字0.01是人们经常用的而另一个常用的值是0.05，那么这就会是5%，如果是这样的话你可以说95%的差异性被保留了，还有其它的一些数值

这些都是比较典型的取值范围 可能从95到99 是人们最为常用的取值范围 对于许多数据集 你可能会惊讶，为了保留99%的差异性通常你可以大幅地降低数据的维度 却还能保留大部分的差异性 

因为大部分现实中的数据，许多特征变量都是高度相关的 所以实际上大量压缩数据是可能的，而且仍然会保留99%或95%的差异性，那么你该如何实现它呢？ 

![14.6.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/*gi0OFXa8UT6Nlde4rXCu.3kli0NRNxeKKELjiiNf2M!/b/dD0BAAAAAAAA&bo=WgRuAgAAAAARFxI!&rf=viewer_4)

你可能会用到这个算法 比如你想选取k的值 我们可以从k=1开始 然后我们再进行主成分分析 我们算出 U_reduce z(1) z(2) 一直到 z(m) 算出所有那些 X_approx(1) 一直到 X_approx(m) 然后我们看一下99%的差异性是否被保留下来了，是的话就选取该k值，k=1 否则我们尝试 k=2 然后我们要重新走一遍这整个过程 检查是否满足这个表达式 这个式子的值是否小于0.01 如果不是再以此类推

但是可以想见 这个过程的效率相当地低 

幸好 你在应用 PCA 时 实际上在这一步它已经给了我们一个可以使计算变得容易很多的量,特别是当你调用svd来计算这些矩阵U S V时

[U,S,V] = svd(Sigma)

当你对协方差的矩阵 Sigma 调用 svd 时 我们还会得到 这个矩阵S 

S是一个正方形矩阵 实际上是一个 n×n 的矩阵 它是一个 对角矩阵 对角线上的元素是 $s_{11}\ s_{22}\ s_{33}$ 一直到 $s_{nn}$,它们是矩阵中 仅有的非零元素 对角以外的其他元素 都是0.

实际上 对于一个给定的k值 这个 平均平方映射误差与数据的总变差相除 的量可以更容易地算出来 那个数值可以通过这个式子计算出来

$$1-\frac{ \sum^k_{i=1}S_{ii} }{ \sum^n_{i=1}S_{ii} }$$ 

接下来我们要做的就是 看一下用上面这个式子算出来的数字 是否小于等于0.01。

所以你要做的 就是慢慢地增大k值 把k值设为1 k值设为2 把k值设为3 以此类推 并检验这个数值 

找出能够确保99%的差异性被保留的最小的k值 

如果这样做那么你只需要调用一次 svd 函数，因为它会给你S矩阵 一旦有了S矩阵 你便可以 通过增加分子上的k值 通过增加分子上的k值 来做这个计算，这样就不用一遍一遍地调用svd函数来检验不同的k值 

因此这一步非常高效 它能够让你 不用一遍一遍地 从头运行 PCA 就能选择k值 你只要运行一次 svd 函数 就能得到所有这些数值 S11 S22 一直到 Snn 然后你就可以 改变这个式子中的k值 来找到能够保留99%的差异性的最小的k值

来总结一下  我在使用 PCA 进行压缩时 通常采用的决定k值的方法是对协方差矩阵调用一次svd函数，然后用这个公式来找出满足这个表达式的最小的k值 

![14.6.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/mmhyH73CMBKvyVFUqRETfG6mREAxujDbqjWaBFWnBHU!/b/dA0BAAAAAAAA&bo=HAbTAgAAAAARF.s!&rf=viewer_4)

顺便说一下 即使你要挑选一些不同的k值,也许你有1000维的数据 我只想要 k=100 如果你想要向别人解释你实现的PCA的性能,实际上是这个数值把它算出来,它会告诉你百分之多少的差异性被保留了下来,如果你把这个数值展现出来,那么熟悉 PCA 的人们 就可以通过它来更好地理解 你用来代表原始数据的100维数据近似得有多好 因为有99%的差异性被保留了 

这就是一个平方投影误差的测量指标

希望对你来说 这是一个选择k值的 有效的过程 如果你将 PCA 应用于 很高维的数据集 常见的 比如一千维的数据 因为数据集的特征变量 通常有较高的相关性,你经常会发现 PCA 能够保留 99%的差异性 或者说95%的差异性 或者某些高百分比的差异性 即使在压缩很大比例的数据的情况下 也会看到这样的现象

## 14.7、 Advice for Apply PAC(应用PCA的建议)

我已经 提到过PCA有时可以用来提高机器学习算法的速度 

在本节中讲解如何在实际操作中来实现,同时列举一些例子 只是希望能够提供一些应用 PCA 的建议 

首先我先介绍如何通过PCA来提高学习算法的速度 这种监督学习算法的提速实际上也是我个人经常通过使用 PCA 来实现的一种功能 

比如说你遇到了一个监督学习问题 注意这个监督学习算法问题有输入x和标签y,假如说你的样本x(i)是非常高维的数据 比如一个10,000维的向量 

比如说其中的一个例子是 你在解决某个计算机 视觉的问题 在这里有 一张100 × 100的图片 那么就是10000 像素 即x(i) 是一个包含了这10000像素强度值的特征向量

像这样 有很高维 的特征向量运行会比较慢 如果你输入10,000维的特征向量到逻辑回归中或者到一个神经网络、支持向量机中或者任何别的算法中会使得学习算法运行得更慢 

幸运的是 通过使用 PCA 我们能够降低数据的维数 从而使得算法能够更加高效地运行 

![14.7.1]()

这就是其中的原理：首先我们需要检查带标签的训练数据集并提取出 输入数据 我们只需要提取出x并暂时把y放在一边 这一步我们会得到 一组无标签的训练集，从 x(1) 到 x(m)，这可能会有 10,000维数据样本，然后我们应用PCA，从中我们会得到一个降维的数据，例如一个降低10倍维度的1000维特征向量

这就给了我们一个新的训练集 所以之前我有这样一个样本 x(1), y(1) 这是我的第一个训练集的输入 现在用 z(1) 来表示 这样我们就有了 一个新的训练集样本 其中z(1)与y(1)是一对儿，同样地 z(2) 对应 y(2) 等等 一直到 z(m) 对 y(m) 

因为现在的训练集 由这样一个 更加低维的数据集所代替 z(1),z(2) 一直到 z(m) 最后 我可以将这个已经降维的数据集 输入到学习算法 或者是将其放入到神经网络/逻辑回归中

这样我们就可以学习出一个假设函数 $h_\theta(z)$ ，把这些低维的 z 作为输入 并作出预测

最后要注意一点 PCA 定义了从x到z的映射,这种从x到z的映射只可以通过在训练集上运行PCA来定义

具体来讲，这种PCA所学习出的这个映射所做的就是计算出一系列的参数进行特征缩放和均值归一化，同时也计算出这样一个降维的矩阵$U_reduced$ 但所有这些东西比如降维矩阵 $U_reduced$ 就是通过学习PCA得到的参数，我们应该只在训练集上拟合这些参数而不是在交叉验证集或者测试集上。因此$U_reduce$矩阵中的数据 就应该 只通过对训练集运行PCA来获得。然后找到了降维矩阵Ureduce 或者找到参数来进行特征缩放或均值归一化以及划分特征的合适的缩放规模

在训练集中找到了所有这些参数后 就可以将同样的对应关系应用到其他样本中了 可能是交叉验证数集样本 或者用在你的测试数据集中 

总结一下 当你在 运行PCA的时候 只是在训练集那一部分来进行的而不是交叉验证集 这就定义了从x到z的映射 然后你就可以将这个映射应用到交叉验证集和测试集中 

通过这个 例子中的这种方式 我们讨论了将数据从上万维降到千维 这实际上并非不切实际 因为对于大多数我们实际面对的数据降维问题 
我们确实可以把数据的维度降维到原来的五分之一或者十分之一 而且依旧保留大部分的方差而且几乎不影响性能 

就分类的精确度而言 数据降维后对学习算法的分类精确度几乎没有什么影响，如果我们将降维用在低维数据上，我们的学习算法会运行得更快

总之  迄今为止我们讨论过的有关PCA的应用中 

第一个是数据压缩 我们可以借此 减少内存或者磁盘空间的使用以存取更多的数据 正如刚刚我们讨论过的 如何使用数据压缩以加快学习算法的例子 在这些应用中 为了选择一个k值 我们将会根据 保留方差的百分比 来确定k值 对于一个学习算法来说 加快应用将会保留99%的方差 在如何选择k值的问题上 这就是一个很典型的问题，也就是说k的选择是一个数据压缩的应用 

然而对于可视化应用来说 

我们通常知道 如何将二维或者三维的数据进行可视化，所以对于可视化应用 我们选择的K值要么等于2 要么等于3 因为我们能画出二维和三维的数据集，我们要根据对于不同的应用来选择K值 

我要提醒的是有一个频繁 被误用的PCA应用 你有时或许能听到其他人这么做  当然我们不希望这样 我只是想提醒你不要这么做 这是一个对PCA不好的应用方面 那就是使用它来避免过拟合 

下面是原因 

如果我们有x(i) 或许x(i)是有n个特征的数据集 如果我们将数据进行压缩 并用压缩后的数据z(i)来代替原始数据 在降维过程中 我们从n个特征降维到k个 比先前的维度低 

在使用PCA进行过拟合处理时PCA做了什么呢？它把某些信息舍弃掉了，并在你对数据标签y值毫不知情的情况下 对数据进行降维，而只使用正则化将会给你一种避免过拟合绝对好的方法，同时正则化 效果也会比PCA更好 因为当你使用线性回归或者逻辑回归 或其他的方法 配合正则化时 这个最小化问题 实际就变成了y值是什么 才不至于 将有用的信息舍弃掉 然而PCA不需要使用到 这些标签y  更容易将有价值信息舍弃 

总之 使用PCA的目的是加速学习算法 但是用它来避免过拟合 却并不是一个好的PCA应用 

最后讲一下PCA的误用 我说PCA是一个非常有用的算法 我经常用它在可视化数据上进行数据压缩 

但我有时候 会看到有些人 把PCA用在了不应当使用的地方 从中我都看到一个共同点 如果某人正在设计机器学习系统 他们或许会写下像这样的计划 让我们设计一个学习系统 得到训练集然后 我要做的是 先运行PCA 然后训练逻辑回归之后在测试数据上进行测试 通常在 一个项目的初期 有些人便直接写出 项目计划而不是说来试试PCA的这四步 

在写下一个使用PCA方法的项目计划前一个非常好的问题是 如果我们在整个项目中 不使用前后有怎样的差别 通常人们不会 去思考这个问题 尤其是当人们提出一个复杂的项目 其中使用了PCA或其它方法 有时 我经常建议大家 在你使用PCA之前 你要知道自己做的是什么 也就是说你想要做什么 这也是你首先 需要在原始数据x(i)上考虑的问题

同时我也建议 一开始不要将 PCA方法就直接放到算法里 先使用原始数据x(i)看看效果 只有一个原因 让我们相信算法出现了问题 那就是你的学习算法收敛地非常缓慢、占用内存或者硬盘空间非常大 所以你想来压缩数据，只有当你有证据或者充足的理由来确定 x(i)效果不好的时候，那么就考虑用PCA来进行压缩数据 

PCA是非常常用的方法之一 也是一种强有力的无监督学习算法，希望你有能力实现PCA算法并用它来实现你的目的 

## Review