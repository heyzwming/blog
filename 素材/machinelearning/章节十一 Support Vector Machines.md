十二、Support Vector Machines(支持向量机)
===

## Large Margin Classification

## 12.1 Optimization Objective( 优化目标)

在监督学习中,许多学习算法的性能都非常类似,因此重要的不是你该选择使用学习算法A还是学习算法B,而更重要的是,应用这些算法时所创建的大量数据。

在应用这些算法时 表现情况通常依赖于你的水平,比如,你为学习算法所设计的特征量的选择,以及如何选择正则化参数，诸如此类的事。

还有一个更加强大的算法广泛的应用于工业界和学术界，它被称为支持向量机(Support Vector Machine),与逻辑回归和神经网络相比,支持向量机,或者简称SVM,在学习复杂的非线性方程时,提供了一种更为清晰 更加强大的方式.

我们从优化目标开始学习这个算法。 

为了描述支持向量机,我将会从逻辑回归开始展示我们如何一点一点修改,来得到本质上的支持向量机.

那么 在逻辑回归中 我们已经熟悉了 这里的假设函数函数$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$ 和右边的Sigmoid激活函数图像 。

然而 为了解释 一些数学知识 我将用 $z$ 表示 $θ$ 转置乘以 $x$ 

$$z = \theta^Tx$$

现在 让一起考虑下 我们想要逻辑回归做什么 如果有一个 y=1 的样本 我的意思是 不管是在训练集中 或是在测试集中 又或者在交叉验证集中 总之是 y=1 现在 我们希望 h(x) 趋近1 因为 我们想要 正确地将此样本分类 

这就意味着 当 h(x) 趋近于1时 $\theta^Tx >> 0$ 这里的 大于大于号 $>>$ 意思是 远远大于0 这是因为这样才到了 该sigmoid函数图像的右边，即逻辑回归的输出将趋近于1 

相反地 如果我们 有另一个样本 即 y=0 我们希望假设函数 的输出值 将趋近于0 这对应于 $z = \theta^Tx <<0$ ,因为此时对应的 假设函数的输出值趋近0 

![12.1.1](http://a4.qpic.cn/psb?/V12umJF70r2BEK/9Zr4QT**hBLlkqo0wiMEPJWjT1J4CM64YBkmffrseME!/b/dAsAAAAAAAAA&ek=1&kp=1&pt=0&bo=LARIAgAAAAARF0I!&tl=3&vuin=904260897&tm=1536048000&sce=60-2-2&rf=viewer_4)

如果你进一步 观察逻辑回归的代价函数 你会发现 每个样本 (x, y) ,都会为总代价函数 增加这样的一项 

因此 对于总代价函数 通常会有对所有的训练样本求和 这个表达式就代表一个单独的训练样本对逻辑回归总体目标函数所做的“贡献”，并且这里还有一个1/m项 

现在 如果我将完整定义的 假设函数的定义 代入$h_\theta(x)$

那么 我们就会得到 每一个训练样本对总体函数的具体贡献,这里先不考虑各项求和，只单纯讨论单独的一项，对逻辑回归总体代价函数所产生的影响。

现在 一起来考虑两种情况 一种是y等于1的情况 一种是y等于0的情况 在第一种情况中 假设 y 等于1 此时 在目标函数中 只需有第一项起作用 因为y等于1时 (1-y) 项将等于0 

![12.1.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/Uu5*4QULFrFb08mC188PTCoVccEtQ2d11Km0A3kJN6w!/b/dOAAAAAAAAAA&bo=HQRUAgAAAAARF28!&rf=viewer_4)

因此 当在$y=1$的样本中时,我们得到 $-log \frac{1}{1+e^{-z}}$ ,这里$z = \theta^Tx$

如果画出关于 z 的函数你会看到左下角的这条曲线.

我们同样可以看到 当$z$增大时,$z$ 对应的值 会变的非常小 对整个代价函数而言 影响也非常小 这也就解释了 为什么 逻辑回归在看见到y=1这样的样本 时 试图将 θ^T*x 设置的非常大 因为 在代价函数中的 这一项会变的非常小 

现在 开始建立支持向量机 我们从这里开始 我们会从这个代价函数$-log \frac{1}{1+e^{-z}}$开始一点一点修改 

让我取这里的 z=1 点 

我先画出将要用的代价函数 新的代价函数将会 水平的从z=1这里到右边 (图外) 

然后我再画一条 同逻辑回归幅度非常相似的直线 但是 在这里 是一条紫红色的直线,那么 到了这里 已经非常接近 逻辑回归中 使用的代价函数了 只是这里是由两条线段组成 即位于右边的水平部分 和位于左边的 直线部分 

这里是在 y=1 的前提下 我们将使用的新的代价函数  你也许能想到 这与逻辑回归的效果很相似。这会让后面的优化问题变得容易，且让支持向量机拥有更好的计算优势。

目前 我们只是讨论了 y=1 的情况 另外 一种情况是当 y=0 时，与y=1时的情况相类似。

那么 现在让我给 这两个方程命名 左边的函数 我称之为 

$cost_1(z)$ 同时 在右边函数 我称它为 $cost_0(z)$

这里的下标是指 在代价函数中 对应的 y=1 和 y=0 的情况 

拥有了这些定义后 现在 我们就开始构建支持向量机 这是我们在逻辑回归中使用 代价函数 J(θ) 也许这个方程 看起来不是非常熟悉 这是因为 之前 有个负号在方程外面 但是 这里我所做的是 将负号移到了 表达式的里面 .

![12.1.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/2LzWFWBXyWq5iiKlIJB79fG0ww3T5V1H7Rogm2GMC0s!/b/dN8AAAAAAAAA&bo=BwQvAgAAAAARFw4!&rf=viewer_4)

对于支持向量机而言 实质上 我们要将$-log(h_\theta(x^{(i)}))$ 替换为 $cost_1(z)$ 也就是$cost_1(θ^Tx)$

因此 对于支持向量机 我们的最小化问题就是

$$\frac{1}{m} \sum_{i=1}^m y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})+\frac{\lambda}{2m}\sum^n_{i=0}\theta_j^2$$

按照支持向量机的惯例 事实上 我们的代价函数 会稍微有些不同 

代价函数的参数表示也会稍微有些不同 首先 我们要 除去 1/m 这一项 当然 这仅仅是 仅仅是由于 人们使用支持向量机时 对比于逻辑回归而言 不同的习惯所致，但是 这也会得出 同样的θ最优值 因为 1/m 仅是个常量 

这里我的意思是 先给你举一个实例 

假定有一最小化问题 即要求当 (u-5)^2+1 取得最小值时的 u 值 

好的 这时最小值为 当 u=5 时取得最小值 

现在 如果我们想要 将这个目标函数 乘上常数10 这里我的最小化问题就变成了 求使得 10×(u-5)^2+10 最小的值u 然而 这里的u值 使得这里最小的u值仍为5 因此 将一些常数 乘以你的最小化项 例如 这里的常数10 这并不会改变 最小化该方程时得到u值 

对于这个原本的代价函数我所做的 是删去常量m 也是相同的 现在 我将目标函数 乘上一个常量 m 并不会改变 取得最小值时的 θ 值 

第二点概念上的变化 我们只是指在使用 支持向量机时 一些如下的标准惯例 而不是逻辑回归 因此 对于逻辑回归 在目标函数中 我们有两项 

第一个是来自于 训练样本的代价 第二个是我们的正则化项 我们不得不去 用这一项来平衡 这就相当于 我们想要最小化 A 加上正则化参数 λ 然后乘以 其他项 B , $A+\lambda B$


这里的 A 表示 原代价函数的第一项 同时 我用 B 表示 原代价函数的第二项 但不包括 λ 

我们要做的是通过设定不同的正则化参数$\lambda$以便能够权衡我们想在多大程度上取适应训练集，也就是最小化A项。还是保证正则参数足够小 也即是 对于B项而言 但对于支持向量机 按照惯例 我们将使用一个不同的参数 为了替换这里使用的 λ 来权衡这两项，我们 依照惯例使用 一个不同的参数 称为C 同时改为优化目标 C×A+B 因此 在逻辑回归中 如果给定 λ 一个非常大的值 意味着给予B更大的权重 而这里 就对应于将C 设定为非常小的值 那么 相应的将会给 B 比给 A 更大的权重 

因此 这只是 一种不同的方式来控制这种权衡 或者一种不同的方法 即用参数来决定是更关心第一项的优化 还是更关心第二项的优化 

当然你也可以 把这里的参数C 考虑成 1/λ 同 1/λ 所扮演的 角色相同 并且这两个方程 或这两个表达式并不相同 因为 C 等于 1/λ 但是也并不全是这样 如果当C等于 1/λ 时 这两个 优化目标应当 得到相同的值 相同的最优值θ 因此  就用它们来代替 那么 我现在删掉这里的 λ 并且用常数 C 来代替这里 

最后我们得到了在支持向量机中我们的整个优化目标函数：
$$C \sum_{i=1}^m y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})+\frac{1}{2}\sum^n_{i=0}\theta_j^2$$

最小化 这个目标函数 得到 SVM 学习到的 参数C 

最后 有别于逻辑回归，逻辑回归输出的是y=1或y=0的概率。但在这里 我们的代价函数 当最小化代价函数 获得参数θ时 支持向量机所做的是：它来直接预测y的值等于1 还是等于0

![12.1.4假设函数的输出形式](http://m.qpic.cn/psb?/V12umJF70r2BEK/XMMwWrG2DSeBU8Bzfqh4fDrz3Uhwupklu8VkkNkvAgI!/b/dNoAAAAAAAAA&bo=pgNrAQAAAAARF.8!&rf=viewer_4)

当 θ^T*x 大于 或者等于0时，假设函数输出1，否则输出0

那么 这就是 支持向量机 数学上的定义 再接下来的视频中 让我们再回去 从直观的角度看看优化目标 实际上是在做什么 以及 SVM 的假设函数 将会学习什么 同时 也会谈谈 如何 做些许修改，来学习更加复杂、非线性的函数 



## 12.2 Large Margin Intuition( 大间隔的直观理解)

人们有时将支持向量机 看做是大间距分类器 在这一部分 我将介绍其中的含义 这有助于我们 直观理解 SVM模型的 假设是什么样的 这是我的支持向量机模型的代价函数 
0:21
在左边这里 我画出了关于 z 的代价函数 cost1(z) 此函数用于正样本 而在右边这里我画出了 
0:30
关于 z 的代价函数 cost0(z) 横轴表示 z 现在让我们考虑一下 最小化这些代价函数的必要条件是什么 
0:39
如果你有一个正样本 y等于1 则只有在 z 大于等于1时 代价函数 cost1(z) 才等于0 换句话说 如果你有一个正样本 我们会希望 θ 转置乘以 x 大于等于1 反之 如果 y 是等于0的 我们观察一下 
1:01
函数cost0(z) 它只有在 z小于等于1 的区间里 函数值为0 这是支持向量机的 一个有趣性质 不是么 事实上 如果你有一个正样本 y等于1 则其实我们仅仅要求 θ 转置乘以 x 大于等于0 
1:22
就能将该样本恰当分出 这是因为如果 θ 转置乘以 x 比0大的话 我们的模型代价函数值为0 类似地 如果你有一个负样本 则仅需要 θ 转置乘以x 小于等于0 就会将负例正确分离 但是 支持向量机的要求更高 不仅仅要能正确分开输入的样本 即不仅仅 要求 θ 转置乘以 x 大于0 我们需要的是 比0值大很多 比如 大于等于1 我也想这个比0小很多 比如我希望它 小于等于-1 这就相当于在支持向量机中嵌入了 一个额外的安全因子 或者说安全的间距因子 当然 逻辑回归 做了类似的事情 但是让我们看一下 在支持向量机中 这个因子会 导致什么结果 
2:14
具体而言 我接下来 会考虑一个特例 我们将这个常数 C 设置成 一个非常大的值 比如我们假设 C的值为100000 或者其它非常大的数 
2:29
然后来观察支持向量机会给出什么结果 如果 C 非常大 则最小化代价函数的时候 
2:36
我们将会很希望 找到一个 使第一项为0的 最优解 
2:44
因此 让我们 尝试在 代价项的第一项 为0的情形下理解 该优化问题 比如我们可以把 C  设置成了非常大的常数 这将给我们 一些关于支持向量机 模型的直观感受 我们已经看到 输入一个训练样本 标签为 y=1 你想令第一项为0 你需要做的是 找到一个 θ 使得 θ 转置乘以 x 大于等于1 类似地 对于一个训练样本 标签为 y=0 为了使 cost0(z) 函数 这个函数 值为0 我们需要 θ 转置 乘以x 的值 
3:37
小于等于-1 因此 现在考虑我们的优化问题 选择参数 使得第一项 等于0 就会导致下面的 优化问题 因为我们将 选择参数使第一项为0 因此这个函数的第一项为0 因此是 C 乘以0 加上二分之一 乘以第二项 这里第一项 是C乘以0 因此可以将其删去 因为我知道它是0 这将遵从以下的约束 θ 转置乘以 x(i) 大于或等于0.5 
4:18
如果 y (i) 
4:22
是等于1 的 θ 转置乘以x(i) 小于等于-1 如果样本i是 一个负样本 这样 当你 求解这个优化问题的时候 当你最小化这个关于变量 θ 的函数的时候 
4:40
你会得到一个非常有趣的决策边界 具体而言 如果你考察 这样一个数据集 其中有正样本 也有负样本 
4:50
可以看到 这个数据集是线性可分的 我的意思是 存在一条直线把正负样本分开 当然有多条不同的直线 可以把 正样本和负样本完全分开 比如 这就是一个决策边界 
5:04
可以把正样本 和负样本分开 但是多多少少这个 看起来并不是非常自然 是么? 或者我们可以画一条更差的决策界 这是另一条决策边界 可以将正样本和负样本分开 但仅仅是勉强分开 这些决策边界看起来都不是特别好的选择 
5:20
支持向量机将会选择 这个黑色的决策边界 
5:29
相较于之前 我用粉色或者绿色 画的决策界 这条黑色的看起来好得多 黑线看起来 是更稳健的决策界 在分离正样本和负样本上它显得的更好 数学上来讲 这是什么意思呢 这条黑线有更大的距离 
5:49
这个距离叫做间距 (margin) 当画出这两条 额外的蓝线 我们看到黑色的决策界 和训练样本之间有更大的最短距离 然而粉线和蓝线 离训练样本就非常近 
6:04
在分离样本的时候就会 比黑线表现差 因此这张图片本身就有一定的误导性 这个距离叫做 支持向量机的 间距 而这是支持向量机 具有鲁棒性的原因 因为它努力 用一个最大间距来分离样本 
6:29
因此支持向量机 有时被称为 大间距分类器 而这其实是 求解上一页幻灯片上优化问题的结果 我知道你也许 想知道 求解上一页幻灯片中的优化问题 为什么会产生这个结果 它是如何产生这个大间距分类器的呢 
6:48
我知道我还没有解释这一点 在下一节视频中 我将会从直观上 略述 为什么 这个优化问题 会产生大间距分类器 总之这个图示 有助于你 理解 支持向量机模型的做法 即努力将正样本和负样本 用最大的间距分开 
7:12
在本节课中 关于大间距分类器 我想讲最后一点 我们将这个大间距分类器 中的正则化因子 常数C 设置的非常大 我记得我将其设置为了100000 因此对这样的一个数据集 也许我们将选择 这样的决策界从而最大间距地 分离开正样本和负样本 
7:37
事实上 支持向量机现在 要比这个大间距分类器所体现的 更成熟 尤其是当你使用 大间距分类器的时候 你的学习算法 会受异常点 (outlier) 的影响 比如我们加入 一个额外的正样本 在这里 如果你加了这个样本 为了将样本 用最大间距分开 
8:02
也许我最终 会得到一条类似这样的决策界 对么? 就是这条粉色的线 仅仅基于 一个异常值 仅仅基于一个样本 就将 我的决策界 从这条黑线变到这条粉线 这实在是不明智的 
8:20
而如果正则化参数 C 设置的非常大 这事实上正是 支持向量机将会做的 它将决策界 从黑线 变到了粉线 但是如果 C 设置的小一点 如果你将 C 设置的不要太大 则你最终会得到 这条黑线 当然数据如果不是线性可分的 如果你在这里 有一些正样本 或者 你在这里有一些负样本 则支持向量机也会将它们恰当分开 因此 大间距分类器的描述 真的 仅仅是从直观上给出了 正则化参数 C 非常大的情形 同时 要提醒你 C 的作用 类似于 λ 分之一 λ 是 我们之前使用过 的正则化参数 这只是C非常大的情形 或者等价地 λ 非常小的情形 你最终会得到 类似粉线这样的决策界 
9:28
但是实际上 应用支持向量机的时候 当 C 不是 非常非常大的时候 
9:34
它可以忽略掉一些异常点的影响 得到更好的决策界 甚至当你的数据不是线性可分的时候 支持向量机也可以给出好的结果 我们稍后会介绍一点 支持向量机的偏差和方差 希望在那时候 关于如何处理参数的这种平衡会变得 更加清晰 我希望 这节课给出了一些 关于为什么支持向量机 被看做大间距分类器的直观理解 它用最大间距将样本区分开 尽管从技术上讲 这只有当 参数C是非常大的时候是真的 
10:10
但是它对于理解支持向量机是有益的 
10:13
本节课中 我们略去了一步 那就是我们在幻灯片中 给出的优化问题 为什么会是这样的 它是如何 得出大间距分类器的 我在本节中没有讲解 在下一节课中 我将略述 这些问题背后的数学原理 来解释 这个优化问题 是如何 得到一个大间距分类器的





## 12.3 Mathematics Behind Large Margin Classification( 大间隔分类器的数学原理)

0:00
在这段视频中 介绍一些 大间隔分类背后的数学原理 
0:05
本节为选学部分 你完全可以跳过它 但是听听这节课可能让你对 支持向量机中的优化问题 以及如何得到 大间距分类器 产生更好的直观理解 
0:21
首先 让我来给大家复习一下 关于向量内积的知识 
0:28
假设我有两个向量 u 和 v 我将它们写在这里 两个都是二维向量 
0:35
我们看一下 u 转置乘以 v 的结果 u 转置乘以 v 也叫做向量 u 和 v 之间的内积 
0:48
由于是二维向量 我可以 将它们画在这个图上 我们说 这就是向量 u 即 在横轴上 取值为某个u1 而在纵轴上 高度是 某个 u2 作为U的 第二个分量 现在 很容易计算的 一个量就是向量 u 的 
1:16
范数 这是双竖线 左边一个 右边一个 表示 u 的范数 即 u 的长度 即向量 u 的欧几里得长度 根据 毕达哥拉斯定理 等于 它等于 u1 平方 加上 u2 平方 开根号 这是向量 u 的长度 它是一个实数 现在你知道了 这个的长度是多少 这个向量的长度写在这里了 我刚刚画的这个 向量的长度就知道了 
1:56
现在让我们回头来看 向量v 因为我们想计算内积 v 是另一个向量 它的两个分量 v1 和 v2 是已知的 
2:08
向量 v 可以画在这里 
2:16
现在让我们 来看看如何计算 u 和 v 之间的内积 这就是具体做法 我们将向量 v 投影到 向量 u 上 我们做一个直角投影 或者说一个90度投影 将其投影到 u 上 
2:36
接下来我度量 这条红线的 长度 我称这条红线的 长度为 p 因此 p 就是长度 或者说是 向量 v 投影到 
2:49
向量 u 上的量 我将它写下来 p 是 v 
2:57
投影到 向量 u 上的 长度 因此可以 将 u 转置乘以 v 写作 p 乘以 u 的范数或者说 u的长度 这是计算内积的一种方法 如果你从几何上 画出 p 的值 同时画出 u 的范数 你也会同样地 计算出内积 答案是一样的 
3:34
对吧 另一个计算公式是  u 转置乘以 v 就是 [u1 u2] 这个一行两列的矩阵 乘以 v 因此 可以得到 u1×v1 加上 u2×v2 
3:51
根据线性代数的知识 这两个公式 会给出同样的结果 
3:57
顺便说一句 u 转置乘以 v 等于 v 转置乘以 u 因此如果你将 u 和 v 交换位置 将 u 投影到 v 上 而不是将 v 投影到 u 上 然后做同样地计算 只是把 u 和 v 的位置交换一下 你事实上可以 得到同样的结果 申明一点 在这个等式中 u 的范数是一个实数 p也是一个实数 因此 u 转置乘以 v 就是两个实数 正常相乘 
4:35
最后一点 需要注意的就是p值 p事实上是有符号的 
4:41
即它可能是正值 也可能是负值 
4:44
我的意思是说 如果 u 是一个类似这样的向量 v 是一个类似这样的向量 
4:52
u 和 v 之间的 夹角大于90度 则如果将 v 投影到 u 上 会得到 这样的一个投影 这是 p 的长度 在这个情形下 我们仍然有 u 转置乘以 v 是等于 p 乘以 u 的范数 唯一一点不同的是 p 在这里是负的 
5:19
在内积计算中 如果 u 和 v 之间的夹角 小于90度 那么那条红线的长度 p 是正值 然而如果 这个夹角 大于90度 则p 将会是负的 就是这个小线段的长度是负的 因此两个向量之间的内积 也是负的 如果它们之间的夹角大于90度 这就是关于向量内积的知识 我们接下来将会 使用这些关于向量内积的 性质 试图来 理解支持向量机 中的目标函数 这就是我们先前给出的 支持向量机模型中的目标函数 为了讲解方便 我做一点简化 仅仅是为了让目标函数 更容易被分析 我接下来忽略掉截距 令 θ0 等于 0 
6:16
这样更容易画示意图 我将特征数 n 置为2 因此我们仅有 
6:23
两个特征 x1 和 x2 
6:26
现在 我们来看一下目标函数 支持向量机的优化目标函数 当我们仅有两个特征 即 n=2 时 这个式子可以写作 二分之一 θ1 平方加上 θ2 平方 我们只有两个参数 θ1 和θ2 
6:45
接下来我重写一下 我将其重写成 二分之一 θ1 平方 加上 θ2 平方 开平方根后再平方 我这么做的根据是 对于任何数 w 
7:00
w的平方根 再取平方 得到的就是 w 本身 因此平方根 然后平方 并不会改变值的大小 
7:08
你可能注意到 括号里面的这一项 是向量 θ 
7:14
的范数 或者说是向量 θ 的长度 我的意思是 如果我们将 向量 θ 写出来 θ1 θ2 那么我刚刚画红线的这一项 就是向量 θ 的长度或范数 这里我们用的是之前 学过的向量范数的定义 
7:36
事实上这就 等于向量 θ 的长度 当然你可以将其写作 θ0 θ1 θ2 如果 θ0等于0  那就是 θ1 θ2 的长度 在这里我将忽略 θ0 将 θ 仅仅写作这样 这样来写 θ θ 的范数 仅仅和 θ1 θ2 有关 但是 数学上不管你是否包含 θ0 其实并没有差别 因此在我们接下来的推导中去掉θ0不会有影响 
8:07
这意味着 我们的目标函数是 等于二分之一 θ范数的平方 
8:16
因此支持向量机 做的全部事情就是 极小化参数向量 θ 范数的平方 或者说 长度的平方 
8:28
现在我将要 看看这些项 θ 转置乘以x 更深入地理解它们的含义 给定参数向量θ 给定一个样本 x 这等于什么呢? 在前一页幻灯片上 我们画出了 在不同情形下 u转置乘以v的示意图 我们将会使用这些概念 θ 和 x(i) 就 类似于 u 和 v 
8:54
让我们看一下示意图 我们考察一个 单一的训练样本 我有一个正样本在这里 用一个叉来表示这个样本 x(i) 意思是 在 
9:12
水平轴上 取值为 x(i)1 在竖直轴上 
9:21
取值为 x(i)2 这就是我画出的训练样本 
9:25
尽管我没有将其 真的看做向量 它事实上 就是一个 始于原点 
9:34
终点位置在这个训练样本点的向量 
9:37
现在 我们有 一个参数向量 我会将它也 画成向量 我将 θ1 画在这里 将 θ2 画在这里 
9:56
那么内积 θ 转置乘以 x(i) 将会是什么呢 使用我们之前的方法 我们计算的方式就是 我将训练样本 投影到参数向量 θ 
10:09
然后我来看一看 这个线段的长度 我将它画成红色 我将它称为 p 上标 (i) 用来表示这是 第 i 个训练样本 
10:24
在参数向量 θ 上的投影 
10:26
根据我们之前幻灯片的内容 我们知道的是 θ 转置乘以 x(i) 等于 就等于 
10:36
p 乘以 向量 θ 的长度 或 范数 
10:43
这就等于 θ1 乘以 x1 
10:47
加上 θ2 x2 这两种方式是等价的 都可以用来计算 θ 和 x(i) 之间的内积 
10:57
好 这告诉了我们什么呢 这里表达的意思是 这个 θ 转置乘以 x(i) 大于等于1 或者小于-1的 约束是可以被 p(i)乘以x大于等于1  这个约束 所代替的 因为 θ 转置乘以 x(i) 等于 p(i) 乘以 θ 的范数 
11:21
将其写入我们的优化目标 我们将会得到 没有了约束 θ 转置乘以x(i) 而变成了 p(i) 乘以 θ 的范数 
11:31
需要提醒一点 我们之前曾讲过 这个优化目标函数可以 被写成二分之一乘以 θ 平方的范数 
11:41
现在让我们考虑 下面这里的 训练样本 现在 继续使用之前的简化 即 θ0 等于0 我们来看一下支持向量机会选择什么样的决策界 
11:55
这是一种选择 我们假设支持向量机会 选择这个决策边界 这不是一个非常好的选择 因为它的间距很小 这个决策界离训练样本的距离很近 
12:09
我们来看一下为什么支持向量机不会选择它 
12:14
对于这样选择的参数 θ 可以看到 参数向量 θ 事实上 是和决策界是90度正交的 因此这个绿色的决策界 对应着一个参数向量 θ  指向这个方向 
12:30
顺便提一句 θ0 等于0 的简化仅仅意味着 决策界必须 通过原点 (0,0) 现在让我们看一下 这对于优化目标函数 意味着什么 
12:45
比如这个样本 
12:47
我们假设它是我的第一个样本 
12:50
x(1) 
12:51
如果我考察这个样本 到参数 θ 的投影 
12:56
这就是投影 
12:57
这个短的红线段 
13:00
就等于p(1) 它非常短 对么 
13:05
类似地 这个样本 
13:09
如果它恰好是 x(2) 是我的第二个训练样本 
13:13
则它到 θ 的投影在这里 
13:18
是因为你犯罪了 因为你做了错事 我将它画成粉色 
13:21
这个短的粉色线段 它是 p(2) 第二个样本到 我的参数向量 θ 
13:30
的投影 
13:33
因此 
13:35
这个投影非常短 
13:36
p(2) 事实上是一个负值 p(2) 是在相反的方向 
13:43
这个向量 和参数向量 θ 的夹角 大于90度 p(2) 的值小于0 
13:50
我们会发现 这些 p(i) 将会是非常小的数 因此当我们考察 优化目标函数的时候 对于正样本而言 我们需要 p(i) 乘以 θ 的范数大于等于1 
14:08
但是如果 p(i) 在这里 如果 p(1) 在这里 非常小 那就意味着 我们需要 θ 的范数 非常大 对么 
14:19
因为如果 p(1) 很小 而我们希望 p(1) 乘以 θ 大于等于1 令其实现的 唯一的办法就是 这两个数较大 如果 p(1) 小 我们就希望 θ 的范数大 
14:34
类似地 对于负样本而言 我们需要 p(2) 
14:39
乘以 θ 的范数 小于等于-1 我们已经在这个样本中 看到 p(2) 会是一个非常小的数 因此唯一的办法 就是 θ 的范数变大 但是我们 的目标函数是 希望 找到一个参数 θ 它的范数 是小的 因此 这看起来 不像是一个好的 参数向量 θ 的选择 相反的 来看一个不同的决策边界 
15:17
比如说 支持向量机选择了 
15:20
这个决策界 
15:22
现在状况会有很大不同 如果这是决策界 这就是 相对应的参数 θ 的方向 因此 在这个 决策界之下 垂直线是决策界 使用线性代数的知识 可以说明 这个绿色的决策界 有一个垂直于它的 向量 θ 现在如果你考察 你的数据在横轴 x 上的投影 
15:48
比如 这个我之前提到的样本 我的样本 x(1) 当我将它投影到横轴x上 或说投影到θ上 就会得到这样的p(1) 
16:00
它的长度是 p(1) 
16:03
另一个样本 那个样本是x(2) 我做同样的投影 我会发现 这是 p(2) 的长度 它是负值 你会注意到 现在 p(1) 和 p(2) 
16:23
这些投影长度 是长多了 如果我们仍然要满足这些约束 p(1) 乘以 θ 的范数 是比1大的 则因为 p(1) 变大了 θ 的范数就可以变小了 
16:41
因此这意味着 通过选择 右边的决策界 而不是左边的那个 支持向量机可以 使参数 θ 的范数 变小很多 因此如果我们想 令 θ 的范数变小 从而令 θ 范数的平方 变小 就能让 支持向量机 选择右边的决策界 
17:02
这就是 
17:05
支持向量机如何能 有效地产生大间距分类的原因 
17:10
看这条绿线 这个绿色的决策界 我们希望 正样本和负样本投影到 θ 的值大 要做到这一点 的唯一方式就是选择这条绿线做决策界 
17:24
这是大间距决策界 来区分开 
17:33
正样本和负样本 这个间距的值 这个间距的值 就是p(1) p(2) p(3) 等等的值 通过让间距变大 通过这些p(1) p(2) p(3) 等等 的值 支持向量机 最终可以找到 一个较小的 θ 范数 这正是支持向量机中最小化目标函数的目的 以上就是为什么 支持向量机最终会找到 大间距分类器的原因 因为它试图极大化这些 p(i) 的范数 它们是 训练样本到决策边界的距离 
18:14
最后一点 我们的推导 自始至终使用了这个简化假设 就是参数 θ0 等于0 就像我之前提到的 这个的作用是 θ0 等于 0 的意思是 我们让决策界 通过原点 让决策界通过原点 就像这样 如果你令 θ0 不是0的话 含义就是你希望 决策界不通过原点 比如这样 我将不会做全部的推导 实际上 支持向量机产生大间距分类器的结论 会被证明同样成立 证明方式是非常类似的 是我们刚刚做的 证明的推广 之前视频中说过 即便 θ0 不等于0 支持向量机要做的事情都是优化这个目标函数 
19:08
对应着 C值非常大的情况 
19:14
但是可以说明的是 即便 θ0 不等于 0 支持向量机 仍然会 找到 正样本和负样本之间的 大间距分隔 总之 我们解释了为什么支持向量机是一个大间距分类器 
19:32
在下一节我们 将开始讨论 如何利用支持向量机的原理 应用它们 建立一个复杂的 非线性分类器 【教育无边界字幕组】翻译: shamolvzhou 校对/审核: 所罗门捷列夫 




### Kernels

## 12.4 Kernels Ⅰ( 核函数 Ⅰ )

在本节课，我将对支持向量机算法做一些改造 以构造复杂的非线性分类器 

主要技巧就是一个称之为核(kernel)的一种东西。

我们来看看核函数是什么 以及如何使用 

如果你有一个训练集 像这个样子 然后你希望拟合一个 非线性的判别边界 来区别正负样本

![12.4.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/jH5ALUukMTrwT9WKcQzzTVgJXQlCmPhcU29hY6TlkUk!/b/dOAAAAAAAAAA&bo=IAO4AQAAAAARF7o!&rf=viewer_4)

一种办法 是构造一个复杂多项式特征的集合，也就是像这样的特征变量集合 这样 你就能得到一个假设$h_\theta(x)$ 如果$θ_0+θ_1x_1 + \dots+$加上其他的多项式特征变量 之和大于0 那么就预测为1 反之 则预测为0 

这种方法 的另一种写法 这里介绍一个新的概念 之后将会用到 我们可以把假设函数 看成是用这个 来计算判别边界 $θ_0+θ_1f_1+ θ_2f_2+θ_3f_3$ 加上其他项 在这里 我将用这几个新的符号 f1 f2 f3等等 来表示一系列我将要计算的 新的特征变量 因此 $f_1=x_1$,$f_2=x_2$, $f_3=x_1x_2$, $f_4=x_1^2$,$f_5=x_2^2$等等 我们之前看到 通过加入这些 高阶项 我们可以得到更多特征变量 

问题是 能不能选择别的特征变量 或者有没有比这些高阶项更好的特征变量 因为 我们并不知道 这些高阶项是不是我们真正需要的 

我们之前谈到 计算机视觉的时候 提到过这时的输入是一个有很多像素的图像 我们看到如果用高阶项作为特征变量 运算量将是非常大的

因此 我们是否有不同的选择 或者是更好的选择来构造特征变量 以用来 嵌入到 假设函数中 事实上 这里有一个可以构造 新特征$f_1,f_2,f_3$的方法

![12.4.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/MPDkacZDgS38j8.AFMoEIItzUUBmV4YakaAAkDn2UWA!/b/dN8AAAAAAAAA&bo=HwO9AQAAAAARF4A!&rf=viewer_4)

在这一行中 我只定义三个 特征变量 但是对于实际问题而言 我们可以定义非常多的特征变量 但是在这里 对于这里的 特征 $x_1,x_2$ 我不打算 把截距$x_0$放在这里，但是对于这里的$x_1 ,x_2$

我打算手动选取一些点 然后将这些点定义为$l^{(1)}$ 再选一个 不同的点 把它定为$l^{(2)}$ 再选第三个点 定为$l^{(3)}$ 

现在 假设我打算 只手动选取三个点 将这三个点作为标记(landmarks): 标记1，标记2，标记3 

接下来我要做的是，像这样定义新的特征变量 给出一个实例x 将第一个特征变量$f_1$ 定义为 一种相似度的度量(measure of the similarity),即度量训练样本x与第一个标记的相似度，我将要用来度量相似度的公式是这样的 

对括号的内容取exp (自然常数e为底的指数函数) 负号 x-l(1) 的长度 平方 除以2倍的 σ 平方 

$$f_1 = similarity(x,l^{(i)}) = e^{(-\frac{ \begin{Vmatrix}
    x-l^{(i)}
\end{Vmatrix}^2 }{2\sigma^2})}$$

记号$\begin{Vmatrix}
    x-l^{(i)}
\end{Vmatrix}$表示向量 w 的长度 因此 这里的 $\begin{Vmatrix}
    x-l^{(i)}
\end{Vmatrix}$ 的意思 就是欧式距离 然后取平方 ,是点 x 与 l(1) 之间的 欧式距离

这是我的第一个特征向量 然后是$f_2\ f_3$类似于$f_1$

这个相似度函数是 用数学术语来说 就是 核函数 这里我所说的核函数 实际上是高斯核函数(Gaussian kernel) 

这个相似度函数Similarity就是核函数，我们这里使用的相似度度量的核函数exp()就是高斯核函数

我们通常用
$$k(x,l^{(i)})$$ 
表示核函数


现在 我们来看看核函数到底可以做什么 为什么这些相似度函数 这些表达式是正确的 

先来看看我们的第一个标记 标记$l^{(1)}$ 这是我之前在图中选取的几个点中的其中一个 

因此x和l(1)之间的核函数相似度是这样表达的 

![12.4.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/AQSswcksozTXKEq8Iuo1orThF0gc0FPLgFmQeQRTiDU!/b/dA0BAAAAAAAA&bo=EwO7AQAAAAARF4o!&rf=viewer_4)

为了保证 你知道 这个分子项是什么 这个分子也可以 写为$\sum^n_{j=1}(x_j-l^{(i)})^2$ 这是向量x和l元素对应的距离 同样地 我忽略了$x_0$ 因此我们暂时先不管截距项$x_0$,$x_0$总是等于1 

那么 你现在明白 这就是你通过计算x和标记之间的相似度得到的核函数 

让我们来看看这个函数计算的是什么 假设x与其中一个标记点$l^{(1)}$非常接近 

那么这个欧式距离 以及这个分子 就会接近于0 

因此f1 这个特征将会接近e的-0次， 然后除以2倍的σ平方 

因此对0取exp 对-0取exp 约等于1 

$$f_1 ≈ exp(-\frac{0^2}{2\sigma^2}) ≈ 1$$

这个距离 不是严格地等于0 但是x越接近于l 那么这个项就会越接近于0 因此f1越接近于1 

相反地 如果x离l(1)越远 那么f1 就等于对一个非常大的数字 的平方除以2倍σ平方 再取exp 然后 对一个负的大数字取exp 接近于0 

$$f_1 ≈ exp(-\frac{large\ number}{2\sigma^2}) ≈ 0$$

因此 这些特征变量的作用是度量 x到标记l的相似度 并且 如果x离l非常相近 那么特征变量f 就接近于1 如果x 离标记l非常远 那么f会约等于0 之前我所画的 那几个标记点 就是 $l^{(1)}\ l^{(2)}\ l^{(3)}$,每一个标记点会定义一个新的特征变量 $f_1\ f_2\ f_3$ 

也就是说 给出一个训练样本 x 我们就能计算三个新的特征变量 f1 f2和f3,这基于我之前给的 三个标记点。  
但是首先 我们来看看这个相似度函数，我们画一些图 来更好地理解这些函数是什么样的 

![12.4.4](http://m.qpic.cn/psb?/V12umJF70r2BEK/lqLwdME489WxwM4TM8vL2dN2ULmvZuXaIR9vSOkYvFY!/b/dJABAAAAAAAA&bo=GQO2AQAAAAARF40!&rf=viewer_4)

比如 假设我们有两个特征x1和x2 假设我们的第一个标记点 是l(1) 位于[3,5] 

假设σ的平方等于1 如果我画出图 就是这样的 这个纵轴 这个曲面的高度,f1的值 再看看水平的坐标x1 x2 

给出一个特定的训练样本x1 x2 即选图上的一个样本 可以看到x1和x2的值 的高度即对应的f1的值(高度)

下面的这个图 内容是一样的 但我用的是一个等高线图 x1为水平轴 x2为竖直轴 那么 底下的这个图 就是这个3D曲面的等值线图 

你会发现 当x等于(3,5)的时候 这个时候 f1就等于1 因为 它在最大值上 所以如果x往旁边移动 离这个点越远 f1的值就越接近0 

这就是特征变量f1 计算的内容 也就是X与第一个标记点 的远近程度 这个值在0到1之间 具体取决于x 距离标记点l(1)到底有多近 

另外我们可以看到改变σ平方的值 能产生多大影响 σ平方是高斯核函数的参数 当你改变它的值的时 你会得到略微不同的结果 

假设我们让$σ^2=0.5$ 你会发现 核函数看起来还是相似的 只是这个突起的宽度变窄了 等值线图也收缩了一些，如果我们从x=(3,5) 开始 往旁边移动 那么特征变量f1 降到0的速度 会变得很快 

$\sigma^2 = 3$的情况与之相反。

因此 讲完了特征变量的定义 我们来看看 我们能得到什么样的预测函数 

![12.4.5](http://m.qpic.cn/psb?/V12umJF70r2BEK/q9UEjVnPwRu4JJ59MXmmwFtJmMkrU.ESvP5tSclbKcU!/b/dOAAAAAAAAAA&bo=IgO3AQAAAAARF7c!&rf=viewer_4)

给定一个训练样本x 我们要计算出三个特征变量 $f_1\ f_2\ f_3$ 预测函数的预测值 会等于1 

如果$θ_0+θ_1f_1+θ_2f_2$ 等等的结果是大于或者等于0的 对于这个特定的例子而言 

假设我们已经找到了一个学习算法 并且假设 我已经得到了 这些参数的值 因此如果$θ_0=-0.5,θ_1=1,θ_2=1,θ_3=0$ 我想要做的是 我想要知道会发生什么 

如果 我们有一个训练样本 它的坐标在这里 这个红点 我画的这个点 假设我们有一个训练样本x 我想知道我的预测函数会给出怎样的预测结果 看看这个公式 

因为我的训练样本x 接近于$l^{(1)}$ 那么$f_1 ≈ 1$ 又因为训练样本$x$ 离$l^{(2)}\ l^{(3)}$都很远 所以 $f_2 ≈ 0$ $f3 ≈ 0$ 

所以 如果我们看看这个公式 $\theta_0+\theta_1*1+\theta_2*? + \theta_3 * 0$ 再把这些$\theta$值代入进去,最后等于0.5 这个值大于等于0 因此 这个点 我们预测出的y值是1 因为大于等于0 

现在我们选择另一个不同的点 假设 我选择了另一个点 我用不同的颜色把它标出来 这个点 如果它是训练样本x 如果你进行和之前相同的计算 你发现$f_1\ f_2\ f_3$都接近于0 

因此 我们得到 $θ_0$加上θ1×f1 加上其他项 最后的结果 会等于-0.5 因为θ0等于-0.5 并且f1 f2 f3都为0 因此最后结果是-0.5 小于0 因此 这个点 我们预测的y值是0 

如果这样做 你自己来对大量的点 进行这样相应的处理 你应该可以确定 如果你有一个训练样本 它非常接近于l(2) 那么通过这个点预测的y值也是1 

实际上 你最后得到的结果是 如果你看看这个边界线 这个区域 我们会发现 对于接近l(1)和l(2)的点 我们的预测值是1 对于远离 l(1)和l(2)的店 对于离这两个标记点非常远的点 我们最后预测的结果 是等于0的 

我们最后会得到 这个预测函数的 判别边界 会像这样 在这个红色的判别边界里面 预测的y值等于1 在这外面预测的y值 等于0 

因此这就是一个 我们如何通过标记点 以及核函数 来训练出非常复杂的非线性 判别边界的方法 就像我刚才画的那个判别边界 

当我们接近两个标记点中任意一个时 预测值就会等于1 否则预测值等于0 

如果这些点离标记点 非常远 这就是核函数这部分 的概念 以及我们如何 在支持向量机中使用它们 我们通过标记点和相似性函数 来定义新的特征变量 从而训练复杂的非线性分类器 

我希望刚才讲的内容能够 帮助你更好的理解核函数的概念 以及我们如何使用它 在支持向量机中定义新的特征变量 

## 12.5 Kernels Ⅱ( 核函数 Ⅱ )

在上一节课中 我谈到过选择标记点 例如 $l^{(1)}\ l^{(2)}\ l^{(3)}$ 这些点使我们能够定义相似度函数,也称之为核函数,在这个例子里,我们的相似度函数为高斯核函数 

![12.5.1](http://a3.qpic.cn/psb?/V12umJF70r2BEK/abLZFdNpHCl7PGVphLqjZzxNGw35qrkIu1OqkGAsUKY!/b/dN4AAAAAAAAA&ek=1&kp=1&pt=0&bo=SAPNAQAAAAARF6c!&tl=3&vuin=904260897&tm=1536062400&sce=60-2-2&rf=viewer_4)

这使我们能够 构造一个预测函数：

$$\theta_0 + \theta_1f_1 + \theta_2f_2+\theta_3f_3>=0$$

但是我们从哪里得到$l^{(1)}\ l^{(2)}\ l^{(3)}$这些标记点？ 在一些复杂的学习问题中 也许我们需要 更多的标记点 而不是我们手选的这三个 

因此 在实际应用时 怎么选取标记点 是机器学习中必须解决的问题 

这是我们的数据集 有一些正样本和一些负样本 我们的想法是 我们将选取样本点,我们拥有的每一个样本点,我们只需要直接使用它们,我们直接将训练样本作为标记点,并用红蓝颜色区分样本点的正负属性。

将左边的样本点图位置不变动地映射到右边的标记点图，最后我们得到每一个标记点 的位置都与 每一个样本点 的位置精确对应的标记点图。

这说明特征函数基本上是在描述：每一个样本距离 样本集中 其他样本的距离。

我们具体的列出 这个过程的大纲 

![12.5.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/Ugm2v6RinRlCEnOMGf6YebPcS86btXfOhCWNEFECvDw!/b/dN0AAAAAAAAA&bo=aQPfAQAAAAARF5Q!&rf=viewer_4)

给定m个训练样本 我将选取与 m个训练样本精确一致 的位置作为我的标记点 

当输入样本x(样本x可以 属于训练集 也可以属于交叉验证集 也可以属于测试集)给定样本x 我们可以计算 这些特征 比如$f_1\ f_2$ 等等 这里 $l^{(1)} = x^{(1)}$ ,剩下标记点 以此类推 

最终我们能到一个特征向量f 我将f1 f2等等 一直写到fm 构造为 特征向量f

$$f = \begin{bmatrix}
    f_1 \\ f_2 \\ \vdots \\ f_m
\end{bmatrix}$$

此外 按照惯例 如果我们需要的话 可以添加额外的特征f0 f0的值始终为1 它与我们之前讨论过的 截距x0的作用相似 

举个例子  假设我们有训练样本$(x^{(i)}, y^{(i)})$ 

这个样本对应的 特征向量可以 这样计算 给定$x^{(i)}$ 我们可以通过相似度函数 

将其映射到,$f_1{(i)}=(x^{(i)},l^{(1)})$在这里 我将整个单词similarity(相似度) 简记为sim 

$f_2{(i)}=(x^{(i)},l^{(2)})$以此类推 最后有$f_m^{(i)}$ 等于$x^{(i)}$与$l^{(m)}$之间的相似度 

在这一列中间的 某个位置 即第i个元素 有一个特征 为fi(i) 为fi(i) 这是x(i)和l(i)之间的 相似度 

这里l(i)就等于 x(i) 所以 fi(i)衡量的是 x(i)与其自身的相似度 

如果你使用高斯核函数的话 这一项等于 exp(-0/(2*sigma^2)) 等于1 所以 对于这个样本来说 其中的某一个特征等于1 

接下来 类似于我们之前的过程 我将这m个特征 合并为一个特征向量 

于是 相比之前用$x^{(i)}$来描述样本 $x^{(i)}$为n维或者n+1维空间的向量 

我们现在可以用 这个特征向量f 来描述我的特征向量 

$$f^{(i)} = \begin{bmatrix}
    f_1^{(i)} \\
    f_2^{(i)} \\
    \vdots \\
    f_m^{(i)} \\
\end{bmatrix}$$

如果有需要的话 我们通常也会加上

$f_0^{(i)} = 1$

那么 这个向量$f$ 就是 我们用于描述训练样本的 特征向量 当给定核函数 和相似度函数后,我们如何使用简单的支持向量机.

如果你已经得到参数 θ 并且想对样本x做出预测。我们先要计算 特征向量f f是m+1维特征向量 

![12.5.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/5vS5laXkazXQkDregqsoCkblBgO1NFXJYuQW25rC3j0!/b/dDwBAAAAAAAA&bo=UgPQAQAAAAARF6A!&rf=viewer_4)

这里之所以有m 是因为我们有m个训练样本 于是就有m个标记点.

我们在 θ^Tf >= 0时 预测y=1 对吧 $\theta^Tf = \theta_0f_0+\theta_1f_1+  \dots+\theta_mf_m$ 所以 参数向量θ 在这里为 m+1维向量 这里有m是因为标记点的个数等于训练点的个数，m就是训练集的大小，所以参数向量θ为m+1维。

以上就是当已知参数θ时 怎么做出预测的过程 怎样得到参数θ呢？ 

你在使用 SVM学习算法时 具体来说就是要求解这个最小化问题 

你需要求出能使这个式子取最小值的参数θ 式子为C乘以这个我们之前见过的代价函数 只是在这里 相比之前使用 $θ^Tx^{(i)}$ 即我们的原始特征 做出预测 我们将替换 特征向量x(i) 并使用这个新的特征向量 

我们使用θ的转置 乘以f(i)来对第i个训练样本 做出预测 我们可以看到 这两个地方(都要做出替换) 通过解决这个最小化问题 我们就能得到支持向量机的参数 

最后一个细节是 对于这个优化问题 我们有 n=m个特征 就在这里 我们拥有的特征个数,即有效的特征量就等于f的维数 

所以 n其实就等于m 如果愿意的话 你也可以认为这是一个求和 这确实就是 j从1到m的累和 

可以这么来看这个问题 你可以想象 n就等于m 因为如果f 不是新的特征向量 那么我们有m+1个特征 额外的1是因为截距的关系 

因此这里 我们仍要j从1累加到n 与我们之前讲过的正则化类似 我们仍然不对θ0 做正则化处理 这就是 j从1累加到m 而不是从0累加到m的原因 

以上 就是支持向量机的学习算法 我在这里 还要讲到 一个数学细节 在支持向量机 实现的过程中 这最后一项与这里写的有细微差别 

其实在实现支持向量机时 你并不需要知道 这个细节 事实上这写下的这个式子 已经给你提供了全部需要的原理但是在支持向量机实现的过程中 这一项 $\sum^m_{j=1}\theta_j$ 这一项可以被重写为$\theta^T\theta$如果我们忽略$θ_0$的话

大多数支持向量机 在实现的时候 其实是替换掉 $\theta^T\theta$ 用 θ 的转置乘以某个矩阵 这依赖于你采用的核函数 再乘以 θ  

$\theta^TM\theta$

这其实是另一种略有区别的距离度量方法 我们用一种略有变化的 度量来取代 

不直接用 θ 的模的平方进行最小化 而是最小化了另一种类似的度量 

这是参数向量θ的变尺度形式 这种变化和核函数相关 这个数学细节 使得支持向量机 能够更有效率的运行 

支持向量机做这种修改的 理由是 这么做可以适应 超大(10000)的训练集 

根据我们之前定义标记点的方法 我们最终有10000个标记点 

θ也随之是10000维的向量 或许这时这么做还可行 但是 当m变得非常非常大时 那么求解 这么多参数 如果m为50,000或者100,000 

此时 利用支持向量机软件包 来解决我写在这里的最小化问题 求解这些参数的成本 会非常高 这些都是数学细节 事实上你没有必要了解这些 

它实际上 细微的修改了最后一项 使得最终的优化目标 与直接最小化θ的模的平方略有区别 如果愿意的话 你可以直接认为 这个具体的实现细节 尽管略微的改变了 优化目标 但是它主要是为了计算效率 所以 你不必要对此有太多担心 

不将核函数这个想法 应用到其他算法比如逻辑回归上的原因：  
事实证明 如果愿意的话 确实可以将核函数 这个想法用于定义特征向量，将标记点之类的技术用于逻辑回归算法，但是用于支持向量机的计算技巧不能较好的推广到其他诸如逻辑回归的算法上，所以将核函数用于逻辑回归时会变得非常的慢

相比之下，这些计算技巧比如具体化技术 对这些细节的修改 以及支持向量软件的实现细节 使得支持向量机 可以和核函数相得益彰 而逻辑回归和核函数 则运行得十分缓慢 更何况它们还不能 使用那些高级优化技巧 

因为这些技巧 是人们专门为 使用核函数的支持向量机开发的 但是这些问题只有 在你亲自实现最小化函数 才会遇到。

我不建议亲自写最小化代价函数的代码 而应该使用人们开发的 成熟的软件包，这些软件包已经包含了那些数值优化技巧。

另外一个 值得说明的问题是 在你使用支持向量机时 怎么选择 支持向量机中的参数？ 

![12.5.4](http://m.qpic.cn/psb?/V12umJF70r2BEK/SgJclWOfEzDyKxP71K5hiffUZCnwdTUmtUpMPPN4l74!/b/dN4AAAAAAAAA&bo=TAPWAQAAAAARF7g!&rf=viewer_4)

在使用支持向量机时的 偏差-方差折中 在使用支持向量机时 其中一个要选择的事情是目标函数中的参数C，C的作用与1/λ相似 λ是逻辑回归算法中 的正则化参数 

所以 大的C对应着 我们以前在逻辑回归 问题中的小的λ 这意味着不使用正则化 就有可能得到一个低偏差但高方差的模型 

如果你使用了 较小的C 这对应着 在逻辑回归中使用较大的 λ,对应着一个高偏差 但是低方差的模型

所以 使用较大C值的模型 为高方差 更倾向于过拟合 而使用较小C值的模型为高偏差 更倾向于欠拟合 

C只是我们要选择的其中一个参数 另外一个要选择的参数是 高斯核函数中的σ^2  

当高斯核函数中的 σ^2偏大时 那么对应的相似度函数 为$exp(-||x-l(i)||^2/(2*σ^2))$ $exp(-||x-l(i)||^2/(2*σ^2))$

exp(-||x-l(i)||^2/(2*σ^2)) 

在这个例子中 如果我们只有一个特征x1 我们在这个位置 有一个标记点 如果σ^2较大 那么高斯核函数 倾向于变得相对平滑 

所以 由于函数平滑 且变化的比较平缓 这会给你的模型 带来较高的偏差和较低的方差 由于高斯核函数变得平缓 就更倾向于得到一个 随着输入x 变化得缓慢的模型 

反之 如果σ^2很小 这是我的标记点 利用其给出特征x1 那么 高斯核函数 即相似度函数会变化的很剧烈,特征的变化会有较大的斜率 和较大的导数 在这种情况下 最终得到的模型会 是低偏差和高方差 

看到这条曲线 

这就是利用核函数的支持向量机算法


## SVMs in Practice

## 12.6 Using An SVM( 使用 SVMs)

支持向量机算法 是一个特定的优化问题 但是就如我在之前提到的 不建议你自己写 软件来求解参数θ 

尽管你不应该去写你自己的SVM优化软件 但是你也需要做几件事儿 

* 首先是 要选择参数C 

* 第二 你也需要选择核函数或你想要使用的相似度函数,其中一个选择是我们选择不用任何核函数 

不用核函数这个作法也叫线性核函数.因此,如果有人说他的 SVM 用了线性核函数,这就意味着他在使用 SVM 时没有用核函数,这种用法的 SVM 只使用了$\theta^Tx$当 $θ_0 + θ_1x_1 + ... + θ_nx_n >= 0$时,预测 y=1 

对线性核函数这个术语 你可以把它理解为 这个版本的 SVM 

![12.6.1](http://a1.qpic.cn/psb?/V12umJF70r2BEK/VaxhqH8nbNdWkU4rCV6YxDNAoPpSlGVWoyWimRMQ5Dw!/b/dPQAAAAAAAAA&ek=1&kp=1&pt=0&bo=NgPIAQAAAAARF9w!&tl=3&vuin=904260897&tm=1536116400&sce=60-2-2&rf=viewer_4)

它只是给你一个标准的线性分类器 

因此对某些问题来说它是一个合理的选择,而且你知道许多软件库,它们可以用来训练不带内核参数的SVM，也叫线性核函数

如果你有大量的特征变量x,x是一个n+1维向量,如果n很大而训练集的样本数m很小,那么你知道你有大量的特征变量 那么如果你已经有 大量的特征值 和很小的训练数据集 也许你应该拟合一个线性的判定边界，不要拟合非常复杂的非线性函数，因为没有足够的数据如果你想在一个高维特征空间试着拟合非常复杂的函数，而你的训练集又很小的话 你可能会**过度拟合**

因此这将是一个合理的设置，在此你可以决定不使用内核参数或一些被叫做线性内核函数的等价物。

对于核函数的第二个选择是 这个高斯核函数

如果你选择这个 那么你要做的另外一个选择是 选择一个参数$σ^2$ 当我们开始讨论一些如何权衡偏差方差的时候谈到过 

如果 σ 的平方很大 那么你就有可能 得到一个较大的误差 较低方差的分类器 

但是如果 σ 的平方很小 那么你就会有较大的方差 较低误差的分类器 

那么什么时候选择高斯核函数呢？

如果你原来的特征变量 x 是 n 维的 如果 n 很小 并且 理想情况下 如果 m 很大 

那么如果我们有 一个二维的训练集 就像我前面讲到的例子一样 那么n等于2 但是我们有相当大的训练集 那么可能你需要用 一个核函数去拟合一个 更复杂的非线性判定边界 那么高斯核函数会是不错的选择

如果你决定使用高斯核函数 那么下面是你需要做的 

![12.6.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/xbvZ7qChZCFaus98V.ri9ijWzJyQlU3iJA2EKw6oJZk!/b/dIQBAAAAAAAA&bo=VgPiAQAAAAARF5Y!&rf=viewer_4)

根据你所用的 支持向量机软件包 它可能需要你 实现一个核函数 或者实现相似度函数 

因此 如果你用 Octave 或者 Matlab 来实现 支持向量机的话 它会要求你提供一个函数 来计算核函数的特定特征 

因此这是对一个特定的 i 计算 fi 

这里的 f 只是 一个单一的实数 也许我应该把它写成 fi 但是你需要做的是 写一个核函数 把这个作为输入 

一个训练样本 或者一个测试样本 不论是哪个 作为输入的是向量 x 然后把标识点 也作为一个输入 在这里我只写了 x1 和 x2 因为标识点也是训练样本 但是你需要做的是 写一个这样的软件 可以将这些x1 x2 作为输入的软件 然后计算它们之间的 这种相似度函数 之后返回一个实数 

因此一些支持向量机的 包需要用户能提供一个核函数 能够输入 x1 x2 并返回一个实数 

从这里开始 它将自动地生成所有特征变量 它自动地 用你写的这个函数 将 x 映射到对应的 f1 f2 一直到 fm并且生成所有的特征值 并从这儿开始训练支持向量机 但是有些时候你却一定要 自己提供这个函数 

如果你使用高斯核函数 一些SVM的实现也会包括高斯核函数 和一些其他的核函数 因为高斯核函数可能是最常见的核函数 

目前看来 高斯核函数和线性核函数确实是 最普遍的核函数 一个实现过程中的注意事项 如果你有大小很不一样 的特征变量，在使用高斯核函数之前 对这些特征变量的大小按比例归一化是很重要的 

原因是：如果假设你在计算 x 和 l 之间的范数 这个式子所算的是 x 和 l 之间的范数 就等于说 计算一个向量 v 这个向量 v=x-l 然后计算向量 v 的范数 这也就是 x 和 l 之间的差 这也就是 x 和 l 之间的差 v 的范数等于

$$\begin{Vmatrix}
    x-l
\end{Vmatrix}$$

$$v = x -l$$

$$\begin{Vmatrix}
    v
\end{Vmatrix}^2 = v^2_1 + v^2_2 ... v^2_n = (x_1-l_1)^2 + (x_2-l_2)^2 + ... + (x_n-l_n)^2$$

现在如果你的特征变量取值范围很不一样 就拿房价预测来举例 如果你的数据 是一些关于房子的数据 如果特征向量 x 的 第一个变量 x1 的取值 在上千平方英尺 的范围内 但是如果你的第二个特征变量 x2 是卧室的数量 且如果它在 一到五个卧室范围内 

那么 $(x_1-l_1)^2$ 将会很大 这有可能上千数值的平方 然而$ (x_2-l_2)^2$ 将会变得很小 在这样的情况下的话 那么在这个式子中 这些间距将几乎 都是由 房子的大小来决定的 从而忽略了卧室的数量 

为了避免这种情况 让向量机得以很好地工作 确实需要对特征变量进行归一化 

这将会保证SVM 能够同等地关注到 所有不同的特征变量 而不是像例子中那样 只关注到房子的大小 而忽略了其他的特征变量 

当你尝试支持向量机时 目前为止你可做的最常用的核函数**线性核函数**(也就是不用核函数) 或者**高斯核函数**

这里有一个警告 不是所有你可能提出来 的相似度函数 都是有效的核函数 高斯核函数 线性核函数 以及有时会用到的其他的核函数,它们全部需要满足一个技术条件 它叫作默塞尔定理 (Mercer's Theorem) 需要满足这个条件的原因是 因为支持向量机算法 或者 SVM 的实现 有许多巧妙的 数值优化技巧,为了有效地求解参数 θ,在最初的设想里，有一个这样的决定 将我们的注意力仅仅限制在 可以满足默塞尔定理的核函数上，这个定理所做的是 确保所有的SVM包 所有的SVM软件包 能够使用 大量的优化方法 并且快速地得到参数 θ 

大多数人最后做的是 要么用线性核函数 要么用高斯核函数 但是还有一些其他核函数 满足默塞尔定理 你可能会遇到其他人使用这些核函数 然而我个人 最后是很少很少使用其他核函数 只是简单提及一下你可能会遇到的其他核函数 

一个是多项式核函数(Polynomial kernel) 

它将 x 和 l 之间的相似度定义为(这里有很多种选择 你可以用)$(X^Tl)^2$

那么这就有一个估计 x 和 l 相似度的估量 如果 x 和 l 相互之间很接近 那么这个内积就会很大 

这是一个有些不寻常的核函数 它并不那么常用 

![12.6.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/VAOcextAE9WDWmVfn72IajYNGrWaxg3e1TngkRx9TBM!/b/dHMBAAAAAAAA&bo=UAPMAQAAAAARF74!&rf=viewer_4)

但是你可能会见到有人使用它 这是多项式核函数的一个变体 另一个是 $(x^Tl)^3$ 

这些都是多项式核函数的例子

$(X^Tl+1)^3$

$(X^Tl+5)^4$

多项式核函数实际上有两个参数 一个是你在$X^Tl$后面要加的数,另一个参数是多项式的次数 参数就是多项式的次数和这些数字 

多项式核函数的更一般形式是 x 转置乘以 l 加上一个常数的某个指数次方 因此这两个 都是多项式核函数的参数 所以多项式核函数几乎总是 或者通常执行的效果 比高斯核函数差一些 所以用得没有那么多 

但是你有可能会碰到 通常它只用在 当 x 和 l 都是严格的非负数时 这样以保证这些 内积值永远不会是负数 

这就可以得到x 和 l 之间非常相似 也许它们之间的内积会很大 它们也有其他的一些性质 但是人们通常用得不多 

那么 根据你所做的 你也有可能会碰到 其它一些更加难懂的核函数 

比如字符串核函数(String kernel) 如果你的输入数据是文本字符串 或者其他类型的字符串 有时会用到这个核函数 还有一些函数 如卡方核函数(chi-square) 直方图交叉核函数(histogram intersection kernel ) 等等 还有一些难懂的核函数 你可以用它们来测量 不同目标之间的相似性 

例如 如果你在尝试 做一些文本分类的问题 在这个问题中 输入变量 x 是一个字符串 我们想要通过 字符串核函数来找到 两个字符串间的相似度但是我 但是我个人很少用 这些更加难懂的核函数 我想我平生可能 用过一次卡方核函数 可能用过一次或者两次 直方图交叉核函数 我实际上没用过字符串核函数 只是以防万一你在其他应用中碰到它们 

最后两个细节 一个是在多类分类中 假说你有4个类别 或者更一般地说是 K 个类别  怎样让 SVM 输出各个类别间合适的判定边界？ 大部分 SVM 包已经内置了 多类分类的函数了 因此如果你用的是那种软件包 你可以直接用内置函数 你可以直接用内置函数 应该可以工作得很好

![12.6.4](http://m.qpic.cn/psb?/V12umJF70r2BEK/nCvIDWLIjNBCZ9mL8S*j.oH5QyDBnWP414qZ0v6XCz0!/b/dAoBAAAAAAAA&bo=PAPYAQAAAAARF8Y!&rf=viewer_4)

另一个方式是 一对多 (one-vs.-all) 方法 这个我们在 讲解逻辑回归的时候讨论过 所以你要做的是 要训练 K 个 SVM 如果你有 K 个类别的话，每一个 SVM 把一个类同其他类区分开 这会给你 K 个参数的向量 

例如θ(1) 它把 y=1 这类和所有其他类别区分开，然后得到第二个参数向量 θ(2)，它是在 y=2 为正类 其他类为负类时得到的，以此类推 一直到参数向量θ(K) 是用于 区分最后一个类别 K 和其他类别的参数向量 

最后 这就与 我们在逻辑回归中用到的 一对多方法一样 在逻辑回归中我们只是 取使得 θ(i) 转置乘以 x 最大的类 i 

以上是多类分类方法 对于更为常见的情况 很有可能的是 不论你使用什么软件包 都很有可能 已经内置了 多类分类的函数功能 因此你不必担心这个 最后 我们从逻辑回归开始 修改了一下代价函数 从而得到了支持向量机 最后我想要在这个视频中讨论一点的是 对这两个算法 你什么时候应该用哪个呢？ 

假设 n 是特征变量的个数 m 是训练样本数 

那么我们什么时候用哪一个呢？ 

![12.6.5](http://m.qpic.cn/psb?/V12umJF70r2BEK/7xwoEo83.IpQ6zw6L02OM2a10MNhKgsto0M.Pqt54wA!/b/dPQAAAAAAAAA&bo=bAPLAQAAAAARF4U!&rf=viewer_4)

如果 n 相对于你的训练集大小来说较大且比训练样本个数m多时，比如说 如果你有一个文本分类的问题 特征向量的维数 有可能是1万 

且如果你的训练集大小 可能是 10 可能最多 1000 想象一下垃圾邮件 的分类问题 在这个问题中 你有1万个特征变量 对应于1万个单词 但是你可能有 10 个训练样本 可能最多 1000 个样本 

如果 n 相对 m 来说 比较大的话 我通常会使用逻辑回归 或者使用 没有核函数的 SVM 或者叫线性核函数 

因为 如果你有许多特征变量 而有相对较小的训练集 一个线性函数就可能工作得不错 而且你也没有 足够的数据 来拟合非常复杂的非线性函数 

现在如果 n 较小 而 m 是中等大小 我的意思是 n 可以取 1 - 1000之间的任何数 1是很小的 也许也会到1000个特征 如果训练样本的数量 可能是从 10 也许是到10,000个样本之间的任何一个值 也许多达5万个样本 所以 m 挺大的 可能是1万 但不是一百万 因此如果 m 大小适中的话 

那么通常高斯核函数的SVM会工作得很好

这个我们在这之前也讨论过 举一个具体的例子 如果你有一个 二维的训练集 所以 n=2 画上很多训练样本 
高斯核函数可以 很好地把正类和负类区分开来 

第三种值得关注的情况是 如果 n 很小 但是 m 很大 如果 n 还是 1到1000之间的数 可能会更大一点 但是如果 m 是 5万 或者更大 大到上百万 5万 10万 一百万 二百万 

你有很大很大的训练集 

如果是这样的情况 那么高斯核函数的支持向量机 运行起来就会很慢

在这种情况下 我经常会做的是 尝试手动地创建 更多的特征变量 然后使用逻辑回归 或者不带核函数的 SVM 

你看这张幻灯片 你看到了逻辑回归 或者不带核函数的 SVM 在这个两个地方都出现了 我把它们放在一起 是有原因的 逻辑回归和不带核函数的 SVM 它们都是非常 相似的算法 不管是逻辑回归 还是不带核函数的 SVM 它们会做相似的事情 并且表现也相似 但是根据你实现的具体情况 其中一个可能会比另一个更加有效 但是如果其中一个算法适用的话 那么另一个算法 也很有可能工作得很好 但是 SVM 的威力 随着你用不同的核函数学习复杂的非线性函数而发挥出来，在这个区间 你有多达1万 或者多达5万的样本 而特征变量的数量 

这是相当大的 那是一个非常常见的区间 也许在这个区间下 高斯核函数的支持向量机会表现得相当突出 你可以做对逻辑回归来说 会困难得多的事情 

最后 神经网络应该在什么时候使用呢？ 对于所有的这些问题 对于所有这些区间 对于所有这些区间 一个设计得很好的神经网络也很可能会非常有效 

它的一个缺点是 或者说有时可能不会使用 神经网络的原因是 对于许多这样的问题 神经网络训练起来可能会很慢 但是如果你有一个非常好的 SVM实现包 它会运行得比较快 比神经网络快很多 

尽管我们在此之前没有证明过，实际上 SVM 的优化问题 是一种凸优化问题 

因此好的 SVM 优化软件包 总是会找到 全局最小值 或者接近它的值 对于SVM 你不需要担心局部最优 

在实际应用中 局部最优 对神经网络来说不是非常大 但是也不小 所以你在使用 SVM 的时候可以少担心一个问题 

根据你的问题 神经网络可能会比 SVM 慢 尤其是在这个区间内 如果你觉得这里给出的参考 看上去有些模糊 如果你在考虑一些问题 

觉得这些参考有一些模糊 我仍然不能完全确定 我是该用这个算法 还是该用那个算法 这个其实没关系 当我遇到机器学习问题时 有时确实不清楚 是不是最好用那个算法 是不是最好用那个算法 但是你在之前的视频中看到的 算法确实很重要 但是通常更重要的是 你有多少数据 你有多熟练 是否擅长做误差分析 和调试学习算法 想出如何设计新的特征变量 想出如何设计新的特征变量 以及找出应该输入给学习算法的其它特征变量等方面 通常这些方面会比 你使用逻辑回归 还是 SVM 这方面更加重要 

但是 已经说过了 SVM 仍然被广泛认为是 最强大的学习算法之一 而且 SVM 在一个区间内 是一个非常有效地学习复杂非线性函数的方法 

因此 我实际上 逻辑回归 神经网络 SVM 加在一起 有了这三个学习算法 有了这三个学习算法 我想你已经具备了 在广泛的应用里 构建最前沿的 机器学习系统的能力 它是你的武器库中的另一个非常强大的工具 它被广泛地应用在很多地方 比如在硅谷 在工业界 在学术等领域来建立许多 高性能的机器学习系统 

### Review
